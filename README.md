# Conditional GAN for Face Sketch Generation

This project implements a Conditional GAN (cGAN) for generating face images from corresponding sketches. The cGAN consists of a Generator and a Discriminator network that are trained adversarially. The generator creates realistic face images conditioned on input sketches, and the discriminator distinguishes between real face images and those generated by the generator.

## Table of Contents
1. [Overview](#overview)
2. [Project Structure](#project-structure)
3. [Installation](#installation)
4. [Usage](#usage)
    - [Training](#training)
    - [Inference](#inference)
5. [Dataset](#dataset)
6. [Model Architecture](#model-architecture)
7. [Training Process](#training-process)
8. [Results](#results)

## Overview
This project uses Conditional GANs to generate face images from input sketches. The Generator takes in a random noise vector (latent space) and a sketch, then outputs a generated face image. The Discriminator is trained to classify between real and fake images, conditioned on the same sketch.

The training dataset consists of pairs of face photos and their corresponding sketches. The model is trained to learn the mapping from sketch to photo using adversarial training.

## Project Structure
```
.
├── checkpoints/            # Directory for saving model checkpoints
├── images/                 # Directory for saving generated images
├── archive/                # Dataset directory containing 'train' and 'val' subfolders
├── main.py                 # Main script for training and inference
├── model.py                # Model definitions (Generator and Discriminator)
├── dataset.py              # Dataset class for loading Face-Sketch pairs
└── README.md               # Project documentation
```

## Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/your-repo-url/cgan-facesketch.git
    cd cgan-facesketch
    ```

2. Install the required Python packages:
    ```bash
    pip install -r requirements.txt
    ```

3. Ensure you have the dataset in the `archive` directory (see [Dataset](#dataset) for details).

4. Set up directories for saving images and model checkpoints:
    ```bash
    mkdir checkpoints images
    ```

## Usage

### Training
To train the Conditional GAN on the Face-Sketch dataset:
1. Ensure you have your dataset directory structured as follows:
    ```
    archive/
    ├── train/
    │   ├── photos/    # Real photos
    │   └── sketches/  # Corresponding sketches
    └── val/
        ├── photos/
        └── sketches/
    ```
2. Run the training script:
    ```bash
    python main.py
    ```

### Inference
To generate a face image from an input sketch using a pre-trained model:
1. Update the `model_path`, `input_image`, and `output_path` variables with the appropriate file paths.
2. Uncomment the inference section in `main.py`:
    ```python
    # For inference
    model_path = "checkpoints/checkpoint_epoch_100.pth"
    input_image = "path/to/input/image.jpg"
    output_path = "path/to/output/sketch.jpg"
    generate_sketch(model_path, input_image, output_path)
    ```
3. Run the script:
    ```bash
    python main.py
    ```

## Dataset
The dataset should be organized into training and validation folders with paired face photos and corresponding sketches. You can use any face-sketch dataset or create your own by manually pairing photos and sketches. Make sure to name the photo and its corresponding sketch with the same file name (e.g., `image_01.jpg` in both `photos` and `sketches` directories).
Used Dataset was from Kaggle which is https://www.kaggle.com/datasets/almightyj/person-face-sketches.
### Example Dataset Structure:
```
archive/
├── train/
│   ├── photos/
│   │   ├── image_01.jpg
│   │   └── image_02.jpg
│   └── sketches/
│       ├── image_01.jpg
│       └── image_02.jpg
└── val/
    ├── photos/
    └── sketches/
```

## Model Architecture

### Generator
The Generator model takes as input:
- A random noise vector (latent space) of size `latent_dim` (default: 100).
- A sketch image as a condition (grayscale, 64x64 resolution).

It outputs a generated face image (3-channel RGB, 64x64 resolution).

### Discriminator
The Discriminator takes as input:
- A face image (real or generated).
- A sketch image as a condition.

It outputs a probability indicating whether the face image is real or generated.

## Training Process

1. **Adversarial Loss:** Binary cross-entropy loss (BCELoss) is used for training both the Generator and Discriminator. The Generator tries to minimize the adversarial loss, while the Discriminator maximizes it.
   
2. **Optimizers:** Adam optimizers are used for both the Generator and Discriminator with learning rates of `0.0002` and betas of `(0.5, 0.999)`.

3. **Checkpointing:** Model checkpoints are saved every few epochs (default: every 10 epochs) to the `checkpoints/` directory.

## Results

During training, generated images are saved periodically to the `images/` directory for visualization. Checkpoint models are saved to the `checkpoints/` directory for future use.
