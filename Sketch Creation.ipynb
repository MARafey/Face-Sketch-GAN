{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b6ce7fba232677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a8008906d6d632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Process condition (sketch) through conv layers\n",
    "        self.condition_processor = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Process noise\n",
    "        self.noise_processor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 16 * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, condition):\n",
    "        # Process condition\n",
    "        processed_condition = self.condition_processor(condition)  # Output: B x 128 x 16 x 16\n",
    "        \n",
    "        # Process noise\n",
    "        processed_noise = self.noise_processor(noise)\n",
    "        processed_noise = processed_noise.view(-1, 128, 16, 16)  # Reshape to match condition\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        combined = torch.cat([processed_noise, processed_condition], dim=1)  # B x 256 x 16 x 16\n",
    "        \n",
    "        # Generate image\n",
    "        output = self.decoder(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c74b8bdb4705b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Process condition (sketch)\n",
    "        self.condition_processor = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Process image\n",
    "        self.image_processor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.combined_processor = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 8 * 8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, condition):\n",
    "        # Process condition and image\n",
    "        processed_condition = self.condition_processor(condition)\n",
    "        processed_image = self.image_processor(image)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([processed_image, processed_condition], dim=1)\n",
    "        features = self.combined_processor(combined)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        features = features.view(features.size(0), -1)\n",
    "        validity = self.classifier(features)\n",
    "        \n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8215c8ee6c674933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceSketchDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.photo_dir = os.path.join(root_dir, split, 'photos')\n",
    "        self.sketch_dir = os.path.join(root_dir, split, 'sketches')\n",
    "\n",
    "        self.photos = sorted([f for f in os.listdir(self.photo_dir) if f.endswith(('.jpg', '.png'))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.photos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        photo_name = self.photos[idx]\n",
    "        photo_path = os.path.join(self.photo_dir, photo_name)\n",
    "        sketch_path = os.path.join(self.sketch_dir, photo_name)\n",
    "\n",
    "        photo = Image.open(photo_path).convert('RGB')\n",
    "        sketch = Image.open(sketch_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            photo = self.transform(photo)\n",
    "            sketch = self.transform(sketch)\n",
    "\n",
    "        return sketch, photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a77c292985270e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(generator, discriminator, epoch, optimizer_G, optimizer_D, path='checkpoints'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "    }, os.path.join(path, f'checkpoint_epoch_{epoch}.pth'))\n",
    "\n",
    "def load_model(generator, discriminator, optimizer_G, optimizer_D, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b9dfd6e89319ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan(generator, discriminator, dataloader, num_epochs, device, save_interval=10):\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sketches, real_imgs) in enumerate(dataloader):\n",
    "            batch_size = real_imgs.size(0)\n",
    "            \n",
    "            real_imgs = real_imgs.to(device)\n",
    "            sketches = sketches.to(device)\n",
    "\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(batch_size, generator.latent_dim).to(device)\n",
    "            gen_imgs = generator(z, sketches)\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs, sketches), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs, sketches), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), sketches), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "        # Save model checkpoints\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(generator, discriminator, epoch + 1, optimizer_G, optimizer_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2016690d55686c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sketch(model_path, image_path, output_path, device='cuda'):\n",
    "    # Initialize model\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    # Load trained model\n",
    "    load_model(generator, discriminator, optimizer_G, optimizer_D, model_path)\n",
    "    generator.to(device)\n",
    "    generator.eval()\n",
    "\n",
    "    # Prepare image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate sketch\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1, generator.latent_dim).to(device)\n",
    "        generated = generator(z, image)\n",
    "\n",
    "    # Save generated image\n",
    "    vutils.save_image(generated, output_path, normalize=True)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb4c1a4c2a7e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    latent_dim = 100\n",
    "    batch_size = 64\n",
    "    num_epochs = 200\n",
    "    image_size = 64\n",
    "    root_dir = \"archive\"  # Update with your dataset path\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create directories for saving results\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = FaceSketchDataset(root_dir, split='train', transform=transform)\n",
    "    val_dataset = FaceSketchDataset(root_dir, split='val', transform=transform)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize models\n",
    "    generator = Generator(latent_dim)\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    # Train the model\n",
    "    train_cgan(generator, discriminator, train_loader, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6fd9f8e766a87cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/323] [D loss: 0.6780] [G loss: 0.8016]\n",
      "[Epoch 0/200] [Batch 100/323] [D loss: 0.3331] [G loss: 3.9636]\n",
      "[Epoch 0/200] [Batch 200/323] [D loss: 0.1987] [G loss: 1.8771]\n",
      "[Epoch 0/200] [Batch 300/323] [D loss: 0.2922] [G loss: 1.5256]\n",
      "[Epoch 1/200] [Batch 0/323] [D loss: 0.2810] [G loss: 3.2125]\n",
      "[Epoch 1/200] [Batch 100/323] [D loss: 0.1683] [G loss: 2.6461]\n",
      "[Epoch 1/200] [Batch 200/323] [D loss: 0.1015] [G loss: 3.7749]\n",
      "[Epoch 1/200] [Batch 300/323] [D loss: 0.2175] [G loss: 2.9279]\n",
      "[Epoch 2/200] [Batch 0/323] [D loss: 0.5609] [G loss: 0.7474]\n",
      "[Epoch 2/200] [Batch 100/323] [D loss: 0.1444] [G loss: 2.7633]\n",
      "[Epoch 2/200] [Batch 200/323] [D loss: 0.1319] [G loss: 2.2718]\n",
      "[Epoch 2/200] [Batch 300/323] [D loss: 0.1677] [G loss: 3.2981]\n",
      "[Epoch 3/200] [Batch 0/323] [D loss: 0.1282] [G loss: 2.5638]\n",
      "[Epoch 3/200] [Batch 100/323] [D loss: 0.0733] [G loss: 3.0565]\n",
      "[Epoch 3/200] [Batch 200/323] [D loss: 0.0904] [G loss: 2.8127]\n",
      "[Epoch 3/200] [Batch 300/323] [D loss: 0.0505] [G loss: 4.5180]\n",
      "[Epoch 4/200] [Batch 0/323] [D loss: 0.0558] [G loss: 3.0608]\n",
      "[Epoch 4/200] [Batch 100/323] [D loss: 0.0442] [G loss: 5.1535]\n",
      "[Epoch 4/200] [Batch 200/323] [D loss: 0.0462] [G loss: 3.8012]\n",
      "[Epoch 4/200] [Batch 300/323] [D loss: 0.0322] [G loss: 4.5750]\n",
      "[Epoch 5/200] [Batch 0/323] [D loss: 0.0393] [G loss: 3.8769]\n",
      "[Epoch 5/200] [Batch 100/323] [D loss: 0.0811] [G loss: 2.3875]\n",
      "[Epoch 5/200] [Batch 200/323] [D loss: 0.0438] [G loss: 3.5777]\n",
      "[Epoch 5/200] [Batch 300/323] [D loss: 0.0723] [G loss: 3.5238]\n",
      "[Epoch 6/200] [Batch 0/323] [D loss: 0.0463] [G loss: 3.5154]\n",
      "[Epoch 6/200] [Batch 100/323] [D loss: 0.0233] [G loss: 5.7584]\n",
      "[Epoch 6/200] [Batch 200/323] [D loss: 0.0741] [G loss: 4.9524]\n",
      "[Epoch 6/200] [Batch 300/323] [D loss: 0.0573] [G loss: 7.3499]\n",
      "[Epoch 7/200] [Batch 0/323] [D loss: 0.0134] [G loss: 5.1264]\n",
      "[Epoch 7/200] [Batch 100/323] [D loss: 0.0076] [G loss: 4.9652]\n",
      "[Epoch 7/200] [Batch 200/323] [D loss: 0.0042] [G loss: 6.4775]\n",
      "[Epoch 7/200] [Batch 300/323] [D loss: 0.5629] [G loss: 1.7355]\n",
      "[Epoch 8/200] [Batch 0/323] [D loss: 0.0749] [G loss: 6.1264]\n",
      "[Epoch 8/200] [Batch 100/323] [D loss: 0.0410] [G loss: 3.4849]\n",
      "[Epoch 8/200] [Batch 200/323] [D loss: 0.0080] [G loss: 5.2205]\n",
      "[Epoch 8/200] [Batch 300/323] [D loss: 0.0081] [G loss: 6.2403]\n",
      "[Epoch 9/200] [Batch 0/323] [D loss: 0.0208] [G loss: 4.1685]\n",
      "[Epoch 9/200] [Batch 100/323] [D loss: 0.0087] [G loss: 7.0076]\n",
      "[Epoch 9/200] [Batch 200/323] [D loss: 0.0046] [G loss: 5.8522]\n",
      "[Epoch 9/200] [Batch 300/323] [D loss: 0.1291] [G loss: 2.2629]\n",
      "[Epoch 10/200] [Batch 0/323] [D loss: 0.0084] [G loss: 5.3563]\n",
      "[Epoch 10/200] [Batch 100/323] [D loss: 0.0640] [G loss: 5.0761]\n",
      "[Epoch 10/200] [Batch 200/323] [D loss: 0.0365] [G loss: 4.4626]\n",
      "[Epoch 10/200] [Batch 300/323] [D loss: 0.0067] [G loss: 6.3175]\n",
      "[Epoch 11/200] [Batch 0/323] [D loss: 0.0452] [G loss: 7.1327]\n",
      "[Epoch 11/200] [Batch 100/323] [D loss: 0.0613] [G loss: 6.5795]\n",
      "[Epoch 11/200] [Batch 200/323] [D loss: 0.0070] [G loss: 5.4103]\n",
      "[Epoch 11/200] [Batch 300/323] [D loss: 0.0537] [G loss: 8.1303]\n",
      "[Epoch 12/200] [Batch 0/323] [D loss: 0.0201] [G loss: 4.1802]\n",
      "[Epoch 12/200] [Batch 100/323] [D loss: 0.0018] [G loss: 6.6280]\n",
      "[Epoch 12/200] [Batch 200/323] [D loss: 0.0014] [G loss: 7.3041]\n",
      "[Epoch 12/200] [Batch 300/323] [D loss: 0.0013] [G loss: 7.7819]\n",
      "[Epoch 13/200] [Batch 0/323] [D loss: 0.0007] [G loss: 8.1765]\n",
      "[Epoch 13/200] [Batch 100/323] [D loss: 0.0211] [G loss: 12.7694]\n",
      "[Epoch 13/200] [Batch 200/323] [D loss: 0.0071] [G loss: 5.2643]\n",
      "[Epoch 13/200] [Batch 300/323] [D loss: 0.0025] [G loss: 6.4350]\n",
      "[Epoch 14/200] [Batch 0/323] [D loss: 0.0044] [G loss: 7.6279]\n",
      "[Epoch 14/200] [Batch 100/323] [D loss: 0.0020] [G loss: 8.4415]\n",
      "[Epoch 14/200] [Batch 200/323] [D loss: 0.0801] [G loss: 4.5373]\n",
      "[Epoch 14/200] [Batch 300/323] [D loss: 0.0087] [G loss: 5.5356]\n",
      "[Epoch 15/200] [Batch 0/323] [D loss: 0.0149] [G loss: 4.4974]\n",
      "[Epoch 15/200] [Batch 100/323] [D loss: 0.0093] [G loss: 6.0438]\n",
      "[Epoch 15/200] [Batch 200/323] [D loss: 0.0081] [G loss: 5.8008]\n",
      "[Epoch 15/200] [Batch 300/323] [D loss: 0.0040] [G loss: 6.1228]\n",
      "[Epoch 16/200] [Batch 0/323] [D loss: 0.0104] [G loss: 4.7803]\n",
      "[Epoch 16/200] [Batch 100/323] [D loss: 0.0029] [G loss: 7.3507]\n",
      "[Epoch 16/200] [Batch 200/323] [D loss: 0.0156] [G loss: 7.6797]\n",
      "[Epoch 16/200] [Batch 300/323] [D loss: 0.0014] [G loss: 8.7129]\n",
      "[Epoch 17/200] [Batch 0/323] [D loss: 0.0053] [G loss: 5.8629]\n",
      "[Epoch 17/200] [Batch 100/323] [D loss: 0.0125] [G loss: 7.0947]\n",
      "[Epoch 17/200] [Batch 200/323] [D loss: 0.0984] [G loss: 3.8908]\n",
      "[Epoch 17/200] [Batch 300/323] [D loss: 0.0017] [G loss: 7.5768]\n",
      "[Epoch 18/200] [Batch 0/323] [D loss: 0.0509] [G loss: 5.6037]\n",
      "[Epoch 18/200] [Batch 100/323] [D loss: 0.0109] [G loss: 5.1948]\n",
      "[Epoch 18/200] [Batch 200/323] [D loss: 0.0050] [G loss: 10.3524]\n",
      "[Epoch 18/200] [Batch 300/323] [D loss: 0.0006] [G loss: 9.9552]\n",
      "[Epoch 19/200] [Batch 0/323] [D loss: 0.0033] [G loss: 9.8139]\n",
      "[Epoch 19/200] [Batch 100/323] [D loss: 5.5541] [G loss: 11.6642]\n",
      "[Epoch 19/200] [Batch 200/323] [D loss: 0.0103] [G loss: 6.0675]\n",
      "[Epoch 19/200] [Batch 300/323] [D loss: 0.0030] [G loss: 6.6556]\n",
      "[Epoch 20/200] [Batch 0/323] [D loss: 0.0019] [G loss: 8.8756]\n",
      "[Epoch 20/200] [Batch 100/323] [D loss: 0.0074] [G loss: 9.4434]\n",
      "[Epoch 20/200] [Batch 200/323] [D loss: 0.0027] [G loss: 7.0112]\n",
      "[Epoch 20/200] [Batch 300/323] [D loss: 0.0089] [G loss: 5.5705]\n",
      "[Epoch 21/200] [Batch 0/323] [D loss: 0.0035] [G loss: 5.9644]\n",
      "[Epoch 21/200] [Batch 100/323] [D loss: 0.0263] [G loss: 4.2433]\n",
      "[Epoch 21/200] [Batch 200/323] [D loss: 0.0004] [G loss: 8.2529]\n",
      "[Epoch 21/200] [Batch 300/323] [D loss: 0.0010] [G loss: 8.0739]\n",
      "[Epoch 22/200] [Batch 0/323] [D loss: 0.0032] [G loss: 8.0278]\n",
      "[Epoch 22/200] [Batch 100/323] [D loss: 0.0327] [G loss: 11.2547]\n",
      "[Epoch 22/200] [Batch 200/323] [D loss: 0.0009] [G loss: 8.0429]\n",
      "[Epoch 22/200] [Batch 300/323] [D loss: 0.0002] [G loss: 10.1179]\n",
      "[Epoch 23/200] [Batch 0/323] [D loss: 1.3462] [G loss: 0.7961]\n",
      "[Epoch 23/200] [Batch 100/323] [D loss: 0.0634] [G loss: 10.2811]\n",
      "[Epoch 23/200] [Batch 200/323] [D loss: 0.0264] [G loss: 3.8993]\n",
      "[Epoch 23/200] [Batch 300/323] [D loss: 0.0035] [G loss: 9.6100]\n",
      "[Epoch 24/200] [Batch 0/323] [D loss: 0.0007] [G loss: 7.7968]\n",
      "[Epoch 24/200] [Batch 100/323] [D loss: 0.0047] [G loss: 7.8008]\n",
      "[Epoch 24/200] [Batch 200/323] [D loss: 0.0193] [G loss: 6.6134]\n",
      "[Epoch 24/200] [Batch 300/323] [D loss: 0.0007] [G loss: 7.8555]\n",
      "[Epoch 25/200] [Batch 0/323] [D loss: 0.0039] [G loss: 6.5981]\n",
      "[Epoch 25/200] [Batch 100/323] [D loss: 0.0018] [G loss: 7.2228]\n",
      "[Epoch 25/200] [Batch 200/323] [D loss: 0.0005] [G loss: 8.6776]\n",
      "[Epoch 25/200] [Batch 300/323] [D loss: 0.0093] [G loss: 10.8776]\n",
      "[Epoch 26/200] [Batch 0/323] [D loss: 0.0023] [G loss: 8.5242]\n",
      "[Epoch 26/200] [Batch 100/323] [D loss: 0.0068] [G loss: 9.1651]\n",
      "[Epoch 26/200] [Batch 200/323] [D loss: 0.0070] [G loss: 6.0036]\n",
      "[Epoch 26/200] [Batch 300/323] [D loss: 0.0009] [G loss: 9.0223]\n",
      "[Epoch 27/200] [Batch 0/323] [D loss: 0.0006] [G loss: 8.9358]\n",
      "[Epoch 27/200] [Batch 100/323] [D loss: 0.0095] [G loss: 8.6655]\n",
      "[Epoch 27/200] [Batch 200/323] [D loss: 0.0047] [G loss: 11.6369]\n",
      "[Epoch 27/200] [Batch 300/323] [D loss: 0.0197] [G loss: 7.0352]\n",
      "[Epoch 28/200] [Batch 0/323] [D loss: 0.7338] [G loss: 0.7764]\n",
      "[Epoch 28/200] [Batch 100/323] [D loss: 0.0059] [G loss: 11.0719]\n",
      "[Epoch 28/200] [Batch 200/323] [D loss: 0.0047] [G loss: 6.6551]\n",
      "[Epoch 28/200] [Batch 300/323] [D loss: 0.0063] [G loss: 5.9146]\n",
      "[Epoch 29/200] [Batch 0/323] [D loss: 0.0064] [G loss: 6.4862]\n",
      "[Epoch 29/200] [Batch 100/323] [D loss: 0.0039] [G loss: 6.3508]\n",
      "[Epoch 29/200] [Batch 200/323] [D loss: 0.0062] [G loss: 10.3507]\n",
      "[Epoch 29/200] [Batch 300/323] [D loss: 0.0007] [G loss: 10.2234]\n",
      "[Epoch 30/200] [Batch 0/323] [D loss: 0.0052] [G loss: 5.9454]\n",
      "[Epoch 30/200] [Batch 100/323] [D loss: 0.0012] [G loss: 8.3512]\n",
      "[Epoch 30/200] [Batch 200/323] [D loss: 0.0146] [G loss: 13.1078]\n",
      "[Epoch 30/200] [Batch 300/323] [D loss: 0.0048] [G loss: 12.5836]\n",
      "[Epoch 31/200] [Batch 0/323] [D loss: 0.0076] [G loss: 5.4566]\n",
      "[Epoch 31/200] [Batch 100/323] [D loss: 0.0019] [G loss: 10.5062]\n",
      "[Epoch 31/200] [Batch 200/323] [D loss: 0.0019] [G loss: 7.9019]\n",
      "[Epoch 31/200] [Batch 300/323] [D loss: 0.0045] [G loss: 5.9526]\n",
      "[Epoch 32/200] [Batch 0/323] [D loss: 0.0011] [G loss: 9.5457]\n",
      "[Epoch 32/200] [Batch 100/323] [D loss: 0.0032] [G loss: 6.6891]\n",
      "[Epoch 32/200] [Batch 200/323] [D loss: 0.0015] [G loss: 10.7927]\n",
      "[Epoch 32/200] [Batch 300/323] [D loss: 0.0001] [G loss: 12.2235]\n",
      "[Epoch 33/200] [Batch 0/323] [D loss: 0.0031] [G loss: 6.6993]\n",
      "[Epoch 33/200] [Batch 100/323] [D loss: 0.0015] [G loss: 7.0088]\n",
      "[Epoch 33/200] [Batch 200/323] [D loss: 0.0059] [G loss: 6.4803]\n",
      "[Epoch 33/200] [Batch 300/323] [D loss: 0.0018] [G loss: 7.6696]\n",
      "[Epoch 34/200] [Batch 0/323] [D loss: 0.0564] [G loss: 5.7629]\n",
      "[Epoch 34/200] [Batch 100/323] [D loss: 0.0017] [G loss: 11.3705]\n",
      "[Epoch 34/200] [Batch 200/323] [D loss: 0.0021] [G loss: 7.5751]\n",
      "[Epoch 34/200] [Batch 300/323] [D loss: 0.0109] [G loss: 5.5120]\n",
      "[Epoch 35/200] [Batch 0/323] [D loss: 0.0010] [G loss: 12.7963]\n",
      "[Epoch 35/200] [Batch 100/323] [D loss: 0.0086] [G loss: 11.2706]\n",
      "[Epoch 35/200] [Batch 200/323] [D loss: 0.0002] [G loss: 10.2477]\n",
      "[Epoch 35/200] [Batch 300/323] [D loss: 0.0035] [G loss: 13.2423]\n",
      "[Epoch 36/200] [Batch 0/323] [D loss: 0.0055] [G loss: 9.5657]\n",
      "[Epoch 36/200] [Batch 100/323] [D loss: 0.0133] [G loss: 10.5679]\n",
      "[Epoch 36/200] [Batch 200/323] [D loss: 0.0525] [G loss: 4.4363]\n",
      "[Epoch 36/200] [Batch 300/323] [D loss: 0.0246] [G loss: 10.8422]\n",
      "[Epoch 37/200] [Batch 0/323] [D loss: 0.0022] [G loss: 9.1334]\n",
      "[Epoch 37/200] [Batch 100/323] [D loss: 0.0184] [G loss: 4.3420]\n",
      "[Epoch 37/200] [Batch 200/323] [D loss: 0.0096] [G loss: 6.0440]\n",
      "[Epoch 37/200] [Batch 300/323] [D loss: 0.0065] [G loss: 11.6907]\n",
      "[Epoch 38/200] [Batch 0/323] [D loss: 0.0007] [G loss: 10.8273]\n",
      "[Epoch 38/200] [Batch 100/323] [D loss: 0.0004] [G loss: 8.4212]\n",
      "[Epoch 38/200] [Batch 200/323] [D loss: 0.0011] [G loss: 8.4017]\n",
      "[Epoch 38/200] [Batch 300/323] [D loss: 0.0001] [G loss: 11.2102]\n",
      "[Epoch 39/200] [Batch 0/323] [D loss: 0.0015] [G loss: 10.8340]\n",
      "[Epoch 39/200] [Batch 100/323] [D loss: 0.0002] [G loss: 9.9060]\n",
      "[Epoch 39/200] [Batch 200/323] [D loss: 0.0002] [G loss: 11.2185]\n",
      "[Epoch 39/200] [Batch 300/323] [D loss: 0.0009] [G loss: 12.0302]\n",
      "[Epoch 40/200] [Batch 0/323] [D loss: 0.0003] [G loss: 13.4723]\n",
      "[Epoch 40/200] [Batch 100/323] [D loss: 0.0008] [G loss: 8.4265]\n",
      "[Epoch 40/200] [Batch 200/323] [D loss: 0.0021] [G loss: 6.9209]\n",
      "[Epoch 40/200] [Batch 300/323] [D loss: 0.0007] [G loss: 10.6059]\n",
      "[Epoch 41/200] [Batch 0/323] [D loss: 0.0018] [G loss: 7.0604]\n",
      "[Epoch 41/200] [Batch 100/323] [D loss: 0.0006] [G loss: 13.3959]\n",
      "[Epoch 41/200] [Batch 200/323] [D loss: 0.0004] [G loss: 14.4260]\n",
      "[Epoch 41/200] [Batch 300/323] [D loss: 0.0006] [G loss: 11.2654]\n",
      "[Epoch 42/200] [Batch 0/323] [D loss: 0.0005] [G loss: 10.2787]\n",
      "[Epoch 42/200] [Batch 100/323] [D loss: 0.0470] [G loss: 5.4990]\n",
      "[Epoch 42/200] [Batch 200/323] [D loss: 0.0110] [G loss: 5.2718]\n",
      "[Epoch 42/200] [Batch 300/323] [D loss: 0.0014] [G loss: 7.5113]\n",
      "[Epoch 43/200] [Batch 0/323] [D loss: 0.0104] [G loss: 13.0911]\n",
      "[Epoch 43/200] [Batch 100/323] [D loss: 0.0151] [G loss: 5.0059]\n",
      "[Epoch 43/200] [Batch 200/323] [D loss: 0.1372] [G loss: 5.1464]\n",
      "[Epoch 43/200] [Batch 300/323] [D loss: 0.0002] [G loss: 14.7234]\n",
      "[Epoch 44/200] [Batch 0/323] [D loss: 0.0001] [G loss: 11.2385]\n",
      "[Epoch 44/200] [Batch 100/323] [D loss: 0.0003] [G loss: 10.8966]\n",
      "[Epoch 44/200] [Batch 200/323] [D loss: 0.0012] [G loss: 11.7374]\n",
      "[Epoch 44/200] [Batch 300/323] [D loss: 0.2089] [G loss: 2.9212]\n",
      "[Epoch 45/200] [Batch 0/323] [D loss: 0.0377] [G loss: 19.1444]\n",
      "[Epoch 45/200] [Batch 100/323] [D loss: 0.0006] [G loss: 10.5369]\n",
      "[Epoch 45/200] [Batch 200/323] [D loss: 0.0009] [G loss: 13.8663]\n",
      "[Epoch 45/200] [Batch 300/323] [D loss: 0.0021] [G loss: 7.3020]\n",
      "[Epoch 46/200] [Batch 0/323] [D loss: 0.0006] [G loss: 10.2361]\n",
      "[Epoch 46/200] [Batch 100/323] [D loss: 0.0019] [G loss: 7.4819]\n",
      "[Epoch 46/200] [Batch 200/323] [D loss: 0.0003] [G loss: 9.7145]\n",
      "[Epoch 46/200] [Batch 300/323] [D loss: 0.0008] [G loss: 8.2310]\n",
      "[Epoch 47/200] [Batch 0/323] [D loss: 0.0018] [G loss: 7.3836]\n",
      "[Epoch 47/200] [Batch 100/323] [D loss: 0.0002] [G loss: 11.1741]\n",
      "[Epoch 47/200] [Batch 200/323] [D loss: 0.0022] [G loss: 7.3366]\n",
      "[Epoch 47/200] [Batch 300/323] [D loss: 0.0001] [G loss: 10.1968]\n",
      "[Epoch 48/200] [Batch 0/323] [D loss: 0.0001] [G loss: 10.4815]\n",
      "[Epoch 48/200] [Batch 100/323] [D loss: 0.0005] [G loss: 9.3800]\n",
      "[Epoch 48/200] [Batch 200/323] [D loss: 0.0010] [G loss: 10.5903]\n",
      "[Epoch 48/200] [Batch 300/323] [D loss: 0.0001] [G loss: 11.1829]\n",
      "[Epoch 49/200] [Batch 0/323] [D loss: 0.0001] [G loss: 11.0028]\n",
      "[Epoch 49/200] [Batch 100/323] [D loss: 0.0002] [G loss: 9.1447]\n",
      "[Epoch 49/200] [Batch 200/323] [D loss: 0.0004] [G loss: 11.8715]\n",
      "[Epoch 49/200] [Batch 300/323] [D loss: 0.0003] [G loss: 9.9626]\n",
      "[Epoch 50/200] [Batch 0/323] [D loss: 0.0015] [G loss: 9.4559]\n",
      "[Epoch 50/200] [Batch 100/323] [D loss: 0.0308] [G loss: 4.8167]\n",
      "[Epoch 50/200] [Batch 200/323] [D loss: 0.0001] [G loss: 10.7540]\n",
      "[Epoch 50/200] [Batch 300/323] [D loss: 0.2095] [G loss: 17.7028]\n",
      "[Epoch 51/200] [Batch 0/323] [D loss: 0.0320] [G loss: 4.7144]\n",
      "[Epoch 51/200] [Batch 100/323] [D loss: 0.0138] [G loss: 5.0359]\n",
      "[Epoch 51/200] [Batch 200/323] [D loss: 0.0015] [G loss: 8.4753]\n",
      "[Epoch 51/200] [Batch 300/323] [D loss: 0.0018] [G loss: 7.6832]\n",
      "[Epoch 52/200] [Batch 0/323] [D loss: 0.0019] [G loss: 9.8707]\n",
      "[Epoch 52/200] [Batch 100/323] [D loss: 0.0000] [G loss: 16.8490]\n",
      "[Epoch 52/200] [Batch 200/323] [D loss: 0.0010] [G loss: 8.8864]\n",
      "[Epoch 52/200] [Batch 300/323] [D loss: 0.0001] [G loss: 10.8563]\n",
      "[Epoch 53/200] [Batch 0/323] [D loss: 0.0074] [G loss: 6.0714]\n",
      "[Epoch 53/200] [Batch 100/323] [D loss: 0.0000] [G loss: 14.9636]\n",
      "[Epoch 53/200] [Batch 200/323] [D loss: 0.0662] [G loss: 3.8675]\n",
      "[Epoch 53/200] [Batch 300/323] [D loss: 0.0136] [G loss: 11.3551]\n",
      "[Epoch 54/200] [Batch 0/323] [D loss: 0.0007] [G loss: 8.6219]\n",
      "[Epoch 54/200] [Batch 100/323] [D loss: 0.0030] [G loss: 8.7573]\n",
      "[Epoch 54/200] [Batch 200/323] [D loss: 0.0044] [G loss: 7.2263]\n",
      "[Epoch 54/200] [Batch 300/323] [D loss: 0.0097] [G loss: 5.7581]\n",
      "[Epoch 55/200] [Batch 0/323] [D loss: 0.0013] [G loss: 9.1922]\n",
      "[Epoch 55/200] [Batch 100/323] [D loss: 0.0038] [G loss: 10.2734]\n",
      "[Epoch 55/200] [Batch 200/323] [D loss: 0.0005] [G loss: 9.4666]\n",
      "[Epoch 55/200] [Batch 300/323] [D loss: 0.0065] [G loss: 6.2748]\n",
      "[Epoch 56/200] [Batch 0/323] [D loss: 0.0186] [G loss: 5.8597]\n",
      "[Epoch 56/200] [Batch 100/323] [D loss: 0.0005] [G loss: 9.7604]\n",
      "[Epoch 56/200] [Batch 200/323] [D loss: 0.0011] [G loss: 11.9660]\n",
      "[Epoch 56/200] [Batch 300/323] [D loss: 0.0000] [G loss: 13.9015]\n",
      "[Epoch 57/200] [Batch 0/323] [D loss: 0.0010] [G loss: 7.9084]\n",
      "[Epoch 57/200] [Batch 100/323] [D loss: 0.0066] [G loss: 9.9198]\n",
      "[Epoch 57/200] [Batch 200/323] [D loss: 0.0015] [G loss: 9.6883]\n",
      "[Epoch 57/200] [Batch 300/323] [D loss: 0.0008] [G loss: 8.9418]\n",
      "[Epoch 58/200] [Batch 0/323] [D loss: 0.0004] [G loss: 10.7004]\n",
      "[Epoch 58/200] [Batch 100/323] [D loss: 0.0004] [G loss: 9.3744]\n",
      "[Epoch 58/200] [Batch 200/323] [D loss: 0.0004] [G loss: 8.8423]\n",
      "[Epoch 58/200] [Batch 300/323] [D loss: 0.0009] [G loss: 7.7028]\n",
      "[Epoch 59/200] [Batch 0/323] [D loss: 0.0001] [G loss: 10.0510]\n",
      "[Epoch 59/200] [Batch 100/323] [D loss: 0.0005] [G loss: 9.4460]\n",
      "[Epoch 59/200] [Batch 200/323] [D loss: 0.0064] [G loss: 6.1705]\n",
      "[Epoch 59/200] [Batch 300/323] [D loss: 0.0001] [G loss: 10.1193]\n",
      "[Epoch 60/200] [Batch 0/323] [D loss: 0.0004] [G loss: 9.4996]\n",
      "[Epoch 60/200] [Batch 100/323] [D loss: 0.0000] [G loss: 13.5654]\n",
      "[Epoch 60/200] [Batch 200/323] [D loss: 0.0038] [G loss: 7.1424]\n",
      "[Epoch 60/200] [Batch 300/323] [D loss: 0.0009] [G loss: 9.6967]\n",
      "[Epoch 61/200] [Batch 0/323] [D loss: 0.0011] [G loss: 7.9693]\n",
      "[Epoch 61/200] [Batch 100/323] [D loss: 0.0002] [G loss: 12.5824]\n",
      "[Epoch 61/200] [Batch 200/323] [D loss: 0.0580] [G loss: 13.5878]\n",
      "[Epoch 61/200] [Batch 300/323] [D loss: 0.0354] [G loss: 6.6962]\n",
      "[Epoch 62/200] [Batch 0/323] [D loss: 0.0069] [G loss: 7.2964]\n",
      "[Epoch 62/200] [Batch 100/323] [D loss: 0.0009] [G loss: 11.0350]\n",
      "[Epoch 62/200] [Batch 200/323] [D loss: 0.0007] [G loss: 9.4551]\n",
      "[Epoch 62/200] [Batch 300/323] [D loss: 1.4065] [G loss: 26.2855]\n",
      "[Epoch 63/200] [Batch 0/323] [D loss: 0.0577] [G loss: 17.5461]\n",
      "[Epoch 63/200] [Batch 100/323] [D loss: 0.0000] [G loss: 12.7676]\n",
      "[Epoch 63/200] [Batch 200/323] [D loss: 0.0000] [G loss: 12.5503]\n",
      "[Epoch 63/200] [Batch 300/323] [D loss: 0.0023] [G loss: 7.4382]\n",
      "[Epoch 64/200] [Batch 0/323] [D loss: 0.0003] [G loss: 9.3888]\n",
      "[Epoch 64/200] [Batch 100/323] [D loss: 0.0002] [G loss: 11.4408]\n",
      "[Epoch 64/200] [Batch 200/323] [D loss: 0.0001] [G loss: 14.2430]\n",
      "[Epoch 64/200] [Batch 300/323] [D loss: 0.0015] [G loss: 9.9681]\n",
      "[Epoch 65/200] [Batch 0/323] [D loss: 0.0001] [G loss: 10.4206]\n",
      "[Epoch 65/200] [Batch 100/323] [D loss: 0.0001] [G loss: 14.0457]\n",
      "[Epoch 65/200] [Batch 200/323] [D loss: 0.0249] [G loss: 5.0168]\n",
      "[Epoch 65/200] [Batch 300/323] [D loss: 0.0339] [G loss: 5.6098]\n",
      "[Epoch 66/200] [Batch 0/323] [D loss: 0.1031] [G loss: 13.2037]\n",
      "[Epoch 66/200] [Batch 100/323] [D loss: 0.0081] [G loss: 7.8064]\n",
      "[Epoch 66/200] [Batch 200/323] [D loss: 0.0002] [G loss: 10.5437]\n",
      "[Epoch 66/200] [Batch 300/323] [D loss: 0.0006] [G loss: 8.5731]\n",
      "[Epoch 67/200] [Batch 0/323] [D loss: 0.0007] [G loss: 10.8370]\n",
      "[Epoch 67/200] [Batch 100/323] [D loss: 0.0570] [G loss: 7.8532]\n",
      "[Epoch 67/200] [Batch 200/323] [D loss: 0.0018] [G loss: 8.1631]\n",
      "[Epoch 67/200] [Batch 300/323] [D loss: 0.0373] [G loss: 11.3396]\n",
      "[Epoch 68/200] [Batch 0/323] [D loss: 0.0005] [G loss: 9.5823]\n",
      "[Epoch 68/200] [Batch 100/323] [D loss: 0.0045] [G loss: 7.3303]\n",
      "[Epoch 68/200] [Batch 200/323] [D loss: 0.0005] [G loss: 9.9843]\n",
      "[Epoch 68/200] [Batch 300/323] [D loss: 0.0066] [G loss: 7.6992]\n",
      "[Epoch 69/200] [Batch 0/323] [D loss: 0.0003] [G loss: 10.3214]\n",
      "[Epoch 69/200] [Batch 100/323] [D loss: 0.0002] [G loss: 10.9101]\n",
      "[Epoch 69/200] [Batch 200/323] [D loss: 0.0006] [G loss: 8.7870]\n",
      "[Epoch 69/200] [Batch 300/323] [D loss: 0.0252] [G loss: 8.2224]\n",
      "[Epoch 70/200] [Batch 0/323] [D loss: 0.0007] [G loss: 9.9517]\n",
      "[Epoch 70/200] [Batch 100/323] [D loss: 0.1822] [G loss: 8.4781]\n",
      "[Epoch 70/200] [Batch 200/323] [D loss: 0.0003] [G loss: 14.7728]\n",
      "[Epoch 70/200] [Batch 300/323] [D loss: 0.0034] [G loss: 6.9929]\n",
      "[Epoch 71/200] [Batch 0/323] [D loss: 0.0017] [G loss: 8.1803]\n",
      "[Epoch 71/200] [Batch 100/323] [D loss: 0.0003] [G loss: 9.6704]\n",
      "[Epoch 71/200] [Batch 200/323] [D loss: 0.0061] [G loss: 7.0260]\n",
      "[Epoch 71/200] [Batch 300/323] [D loss: 0.0034] [G loss: 7.4807]\n",
      "[Epoch 72/200] [Batch 0/323] [D loss: 0.0047] [G loss: 6.9454]\n",
      "[Epoch 72/200] [Batch 100/323] [D loss: 0.0008] [G loss: 9.7089]\n",
      "[Epoch 72/200] [Batch 200/323] [D loss: 0.0005] [G loss: 11.7588]\n",
      "[Epoch 72/200] [Batch 300/323] [D loss: 0.0017] [G loss: 10.7744]\n",
      "[Epoch 73/200] [Batch 0/323] [D loss: 0.0010] [G loss: 10.9738]\n",
      "[Epoch 73/200] [Batch 100/323] [D loss: 0.0002] [G loss: 11.7954]\n",
      "[Epoch 73/200] [Batch 200/323] [D loss: 0.0002] [G loss: 11.1561]\n",
      "[Epoch 73/200] [Batch 300/323] [D loss: 0.0001] [G loss: 10.5779]\n",
      "[Epoch 74/200] [Batch 0/323] [D loss: 0.0003] [G loss: 9.7828]\n",
      "[Epoch 74/200] [Batch 100/323] [D loss: 0.0237] [G loss: 4.8977]\n",
      "[Epoch 74/200] [Batch 200/323] [D loss: 0.0012] [G loss: 9.5791]\n",
      "[Epoch 74/200] [Batch 300/323] [D loss: 0.0002] [G loss: 10.3180]\n",
      "[Epoch 75/200] [Batch 0/323] [D loss: 0.0117] [G loss: 7.2884]\n",
      "[Epoch 75/200] [Batch 100/323] [D loss: 0.0095] [G loss: 7.5619]\n",
      "[Epoch 75/200] [Batch 200/323] [D loss: 0.0091] [G loss: 6.7441]\n",
      "[Epoch 75/200] [Batch 300/323] [D loss: 0.0001] [G loss: 13.1704]\n",
      "[Epoch 76/200] [Batch 0/323] [D loss: 0.0014] [G loss: 12.6613]\n",
      "[Epoch 76/200] [Batch 100/323] [D loss: 0.0001] [G loss: 10.4940]\n",
      "[Epoch 76/200] [Batch 200/323] [D loss: 0.0008] [G loss: 9.5334]\n",
      "[Epoch 76/200] [Batch 300/323] [D loss: 0.0009] [G loss: 9.3179]\n",
      "[Epoch 77/200] [Batch 0/323] [D loss: 0.0061] [G loss: 7.6875]\n",
      "[Epoch 77/200] [Batch 100/323] [D loss: 0.0182] [G loss: 5.7259]\n",
      "[Epoch 77/200] [Batch 200/323] [D loss: 0.0003] [G loss: 13.5010]\n",
      "[Epoch 77/200] [Batch 300/323] [D loss: 0.0014] [G loss: 10.9073]\n",
      "[Epoch 78/200] [Batch 0/323] [D loss: 0.0001] [G loss: 10.7642]\n",
      "[Epoch 78/200] [Batch 100/323] [D loss: 0.0341] [G loss: 5.1992]\n",
      "[Epoch 78/200] [Batch 200/323] [D loss: 0.3166] [G loss: 2.1865]\n",
      "[Epoch 78/200] [Batch 300/323] [D loss: 0.0002] [G loss: 12.8563]\n",
      "[Epoch 79/200] [Batch 0/323] [D loss: 0.0007] [G loss: 8.5130]\n",
      "[Epoch 79/200] [Batch 100/323] [D loss: 0.0002] [G loss: 13.3314]\n",
      "[Epoch 79/200] [Batch 200/323] [D loss: 0.0013] [G loss: 9.2379]\n",
      "[Epoch 79/200] [Batch 300/323] [D loss: 0.0004] [G loss: 11.3302]\n",
      "[Epoch 80/200] [Batch 0/323] [D loss: 0.0000] [G loss: 13.2962]\n",
      "[Epoch 80/200] [Batch 100/323] [D loss: 0.1022] [G loss: 4.1658]\n",
      "[Epoch 80/200] [Batch 200/323] [D loss: 0.0003] [G loss: 13.3863]\n",
      "[Epoch 80/200] [Batch 300/323] [D loss: 0.0091] [G loss: 6.1600]\n",
      "[Epoch 81/200] [Batch 0/323] [D loss: 0.0008] [G loss: 9.9604]\n",
      "[Epoch 81/200] [Batch 100/323] [D loss: 0.0002] [G loss: 12.7238]\n",
      "[Epoch 81/200] [Batch 200/323] [D loss: 0.0002] [G loss: 10.6560]\n",
      "[Epoch 81/200] [Batch 300/323] [D loss: 0.0023] [G loss: 7.5773]\n",
      "[Epoch 82/200] [Batch 0/323] [D loss: 0.0001] [G loss: 14.2903]\n",
      "[Epoch 82/200] [Batch 100/323] [D loss: 0.0019] [G loss: 8.2408]\n",
      "[Epoch 82/200] [Batch 200/323] [D loss: 0.0000] [G loss: 15.9111]\n",
      "[Epoch 82/200] [Batch 300/323] [D loss: 0.0002] [G loss: 10.3188]\n",
      "[Epoch 83/200] [Batch 0/323] [D loss: 3.0129] [G loss: 27.8110]\n",
      "[Epoch 83/200] [Batch 100/323] [D loss: 0.0064] [G loss: 11.8093]\n",
      "[Epoch 83/200] [Batch 200/323] [D loss: 0.0001] [G loss: 14.0521]\n",
      "[Epoch 83/200] [Batch 300/323] [D loss: 0.0010] [G loss: 10.1094]\n",
      "[Epoch 84/200] [Batch 0/323] [D loss: 0.0002] [G loss: 11.9292]\n",
      "[Epoch 84/200] [Batch 100/323] [D loss: 0.0000] [G loss: 14.6878]\n",
      "[Epoch 84/200] [Batch 200/323] [D loss: 0.0001] [G loss: 11.0132]\n",
      "[Epoch 84/200] [Batch 300/323] [D loss: 0.0000] [G loss: 13.7890]\n",
      "[Epoch 85/200] [Batch 0/323] [D loss: 0.0006] [G loss: 10.8612]\n",
      "[Epoch 85/200] [Batch 100/323] [D loss: 0.0013] [G loss: 8.8009]\n",
      "[Epoch 85/200] [Batch 200/323] [D loss: 0.0007] [G loss: 16.6521]\n",
      "[Epoch 85/200] [Batch 300/323] [D loss: 0.0001] [G loss: 12.5472]\n",
      "[Epoch 86/200] [Batch 0/323] [D loss: 0.0012] [G loss: 8.6967]\n",
      "[Epoch 86/200] [Batch 100/323] [D loss: 0.0000] [G loss: 12.7140]\n",
      "[Epoch 86/200] [Batch 200/323] [D loss: 0.0009] [G loss: 8.8377]\n",
      "[Epoch 86/200] [Batch 300/323] [D loss: 0.0000] [G loss: 13.7233]\n",
      "[Epoch 87/200] [Batch 0/323] [D loss: 0.0001] [G loss: 13.0524]\n",
      "[Epoch 87/200] [Batch 100/323] [D loss: 0.0060] [G loss: 12.2401]\n",
      "[Epoch 87/200] [Batch 200/323] [D loss: 0.0002] [G loss: 11.4899]\n",
      "[Epoch 87/200] [Batch 300/323] [D loss: 0.0003] [G loss: 9.8266]\n",
      "[Epoch 88/200] [Batch 0/323] [D loss: 0.0123] [G loss: 7.9278]\n",
      "[Epoch 88/200] [Batch 100/323] [D loss: 0.0659] [G loss: 11.4912]\n",
      "[Epoch 88/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.3464]\n",
      "[Epoch 88/200] [Batch 300/323] [D loss: 0.0006] [G loss: 9.8715]\n",
      "[Epoch 89/200] [Batch 0/323] [D loss: 0.0000] [G loss: 11.6005]\n",
      "[Epoch 89/200] [Batch 100/323] [D loss: 0.0005] [G loss: 15.3520]\n",
      "[Epoch 89/200] [Batch 200/323] [D loss: 0.0026] [G loss: 15.2238]\n",
      "[Epoch 89/200] [Batch 300/323] [D loss: 0.0057] [G loss: 6.3853]\n",
      "[Epoch 90/200] [Batch 0/323] [D loss: 0.0011] [G loss: 8.6329]\n",
      "[Epoch 90/200] [Batch 100/323] [D loss: 0.0018] [G loss: 17.9199]\n",
      "[Epoch 90/200] [Batch 200/323] [D loss: 0.0014] [G loss: 13.4259]\n",
      "[Epoch 90/200] [Batch 300/323] [D loss: 0.0009] [G loss: 10.6859]\n",
      "[Epoch 91/200] [Batch 0/323] [D loss: 0.0042] [G loss: 8.4956]\n",
      "[Epoch 91/200] [Batch 100/323] [D loss: 0.0159] [G loss: 8.8493]\n",
      "[Epoch 91/200] [Batch 200/323] [D loss: 0.0621] [G loss: 4.7612]\n",
      "[Epoch 91/200] [Batch 300/323] [D loss: 0.0072] [G loss: 7.5846]\n",
      "[Epoch 92/200] [Batch 0/323] [D loss: 0.0077] [G loss: 8.6236]\n",
      "[Epoch 92/200] [Batch 100/323] [D loss: 0.0024] [G loss: 9.2739]\n",
      "[Epoch 92/200] [Batch 200/323] [D loss: 0.0026] [G loss: 11.0750]\n",
      "[Epoch 92/200] [Batch 300/323] [D loss: 0.0020] [G loss: 8.7089]\n",
      "[Epoch 93/200] [Batch 0/323] [D loss: 0.0002] [G loss: 11.2224]\n",
      "[Epoch 93/200] [Batch 100/323] [D loss: 0.0001] [G loss: 10.6954]\n",
      "[Epoch 93/200] [Batch 200/323] [D loss: 0.0182] [G loss: 6.1976]\n",
      "[Epoch 93/200] [Batch 300/323] [D loss: 0.0001] [G loss: 13.1567]\n",
      "[Epoch 94/200] [Batch 0/323] [D loss: 0.0025] [G loss: 9.0809]\n",
      "[Epoch 94/200] [Batch 100/323] [D loss: 0.0004] [G loss: 9.1002]\n",
      "[Epoch 94/200] [Batch 200/323] [D loss: 0.0001] [G loss: 12.5265]\n",
      "[Epoch 94/200] [Batch 300/323] [D loss: 0.0027] [G loss: 8.4525]\n",
      "[Epoch 95/200] [Batch 0/323] [D loss: 0.0010] [G loss: 10.1720]\n",
      "[Epoch 95/200] [Batch 100/323] [D loss: 0.0010] [G loss: 14.5530]\n",
      "[Epoch 95/200] [Batch 200/323] [D loss: 0.0050] [G loss: 11.9447]\n",
      "[Epoch 95/200] [Batch 300/323] [D loss: 0.0000] [G loss: 11.7957]\n",
      "[Epoch 96/200] [Batch 0/323] [D loss: 0.0231] [G loss: 5.8737]\n",
      "[Epoch 96/200] [Batch 100/323] [D loss: 0.0001] [G loss: 11.5451]\n",
      "[Epoch 96/200] [Batch 200/323] [D loss: 0.0056] [G loss: 7.1050]\n",
      "[Epoch 96/200] [Batch 300/323] [D loss: 0.0046] [G loss: 8.8131]\n",
      "[Epoch 97/200] [Batch 0/323] [D loss: 0.0020] [G loss: 8.4807]\n",
      "[Epoch 97/200] [Batch 100/323] [D loss: 0.0046] [G loss: 7.2018]\n",
      "[Epoch 97/200] [Batch 200/323] [D loss: 0.0000] [G loss: 14.2458]\n",
      "[Epoch 97/200] [Batch 300/323] [D loss: 0.0029] [G loss: 10.3445]\n",
      "[Epoch 98/200] [Batch 0/323] [D loss: 0.0001] [G loss: 10.8306]\n",
      "[Epoch 98/200] [Batch 100/323] [D loss: 0.0003] [G loss: 11.8240]\n",
      "[Epoch 98/200] [Batch 200/323] [D loss: 0.0001] [G loss: 14.6459]\n",
      "[Epoch 98/200] [Batch 300/323] [D loss: 0.0005] [G loss: 10.2269]\n",
      "[Epoch 99/200] [Batch 0/323] [D loss: 0.0105] [G loss: 15.9033]\n",
      "[Epoch 99/200] [Batch 100/323] [D loss: 0.0079] [G loss: 6.5176]\n",
      "[Epoch 99/200] [Batch 200/323] [D loss: 0.0061] [G loss: 16.6290]\n",
      "[Epoch 99/200] [Batch 300/323] [D loss: 0.0001] [G loss: 17.5912]\n",
      "[Epoch 100/200] [Batch 0/323] [D loss: 0.0002] [G loss: 11.2261]\n",
      "[Epoch 100/200] [Batch 100/323] [D loss: 0.0007] [G loss: 9.7088]\n",
      "[Epoch 100/200] [Batch 200/323] [D loss: 0.0000] [G loss: 14.2720]\n",
      "[Epoch 100/200] [Batch 300/323] [D loss: 0.0001] [G loss: 13.4424]\n",
      "[Epoch 101/200] [Batch 0/323] [D loss: 0.0007] [G loss: 10.6298]\n",
      "[Epoch 101/200] [Batch 100/323] [D loss: 0.0008] [G loss: 13.1502]\n",
      "[Epoch 101/200] [Batch 200/323] [D loss: 0.0005] [G loss: 11.1303]\n",
      "[Epoch 101/200] [Batch 300/323] [D loss: 0.0002] [G loss: 16.2595]\n",
      "[Epoch 102/200] [Batch 0/323] [D loss: 0.0022] [G loss: 16.6145]\n",
      "[Epoch 102/200] [Batch 100/323] [D loss: 0.0000] [G loss: 13.7192]\n",
      "[Epoch 102/200] [Batch 200/323] [D loss: 0.0000] [G loss: 12.9466]\n",
      "[Epoch 102/200] [Batch 300/323] [D loss: 0.0002] [G loss: 17.4852]\n",
      "[Epoch 103/200] [Batch 0/323] [D loss: 0.0000] [G loss: 18.8983]\n",
      "[Epoch 103/200] [Batch 100/323] [D loss: 0.0000] [G loss: 17.7522]\n",
      "[Epoch 103/200] [Batch 200/323] [D loss: 0.0000] [G loss: 19.8195]\n",
      "[Epoch 103/200] [Batch 300/323] [D loss: 0.0007] [G loss: 10.7941]\n",
      "[Epoch 104/200] [Batch 0/323] [D loss: 0.0007] [G loss: 11.4191]\n",
      "[Epoch 104/200] [Batch 100/323] [D loss: 0.0004] [G loss: 13.7208]\n",
      "[Epoch 104/200] [Batch 200/323] [D loss: 0.0002] [G loss: 12.7646]\n",
      "[Epoch 104/200] [Batch 300/323] [D loss: 0.0003] [G loss: 9.7350]\n",
      "[Epoch 105/200] [Batch 0/323] [D loss: 0.0004] [G loss: 9.6263]\n",
      "[Epoch 105/200] [Batch 100/323] [D loss: 0.0266] [G loss: 7.1499]\n",
      "[Epoch 105/200] [Batch 200/323] [D loss: 0.0023] [G loss: 11.4696]\n",
      "[Epoch 105/200] [Batch 300/323] [D loss: 0.0942] [G loss: 9.6942]\n",
      "[Epoch 106/200] [Batch 0/323] [D loss: 0.0003] [G loss: 11.4884]\n",
      "[Epoch 106/200] [Batch 100/323] [D loss: 0.0002] [G loss: 12.0376]\n",
      "[Epoch 106/200] [Batch 200/323] [D loss: 0.0010] [G loss: 17.4645]\n",
      "[Epoch 106/200] [Batch 300/323] [D loss: 0.0007] [G loss: 10.1942]\n",
      "[Epoch 107/200] [Batch 0/323] [D loss: 0.0006] [G loss: 12.2414]\n",
      "[Epoch 107/200] [Batch 100/323] [D loss: 0.0014] [G loss: 13.5706]\n",
      "[Epoch 107/200] [Batch 200/323] [D loss: 0.0000] [G loss: 17.3926]\n",
      "[Epoch 107/200] [Batch 300/323] [D loss: 0.0000] [G loss: 13.6196]\n",
      "[Epoch 108/200] [Batch 0/323] [D loss: 0.0019] [G loss: 7.3660]\n",
      "[Epoch 108/200] [Batch 100/323] [D loss: 0.0005] [G loss: 13.8097]\n",
      "[Epoch 108/200] [Batch 200/323] [D loss: 0.0053] [G loss: 16.7111]\n",
      "[Epoch 108/200] [Batch 300/323] [D loss: 0.0101] [G loss: 14.0061]\n",
      "[Epoch 109/200] [Batch 0/323] [D loss: 0.0070] [G loss: 8.7975]\n",
      "[Epoch 109/200] [Batch 100/323] [D loss: 0.0200] [G loss: 13.8660]\n",
      "[Epoch 109/200] [Batch 200/323] [D loss: 0.0005] [G loss: 11.6614]\n",
      "[Epoch 109/200] [Batch 300/323] [D loss: 0.0238] [G loss: 5.8066]\n",
      "[Epoch 110/200] [Batch 0/323] [D loss: 0.0003] [G loss: 12.1417]\n",
      "[Epoch 110/200] [Batch 100/323] [D loss: 0.0011] [G loss: 11.2128]\n",
      "[Epoch 110/200] [Batch 200/323] [D loss: 0.0025] [G loss: 16.0527]\n",
      "[Epoch 110/200] [Batch 300/323] [D loss: 0.0114] [G loss: 6.8666]\n",
      "[Epoch 111/200] [Batch 0/323] [D loss: 0.0019] [G loss: 9.8280]\n",
      "[Epoch 111/200] [Batch 100/323] [D loss: 0.0027] [G loss: 10.0136]\n",
      "[Epoch 111/200] [Batch 200/323] [D loss: 0.0006] [G loss: 9.9776]\n",
      "[Epoch 111/200] [Batch 300/323] [D loss: 0.0006] [G loss: 12.2371]\n",
      "[Epoch 112/200] [Batch 0/323] [D loss: 0.0018] [G loss: 8.4004]\n",
      "[Epoch 112/200] [Batch 100/323] [D loss: 0.0002] [G loss: 16.5888]\n",
      "[Epoch 112/200] [Batch 200/323] [D loss: 0.0004] [G loss: 13.6413]\n",
      "[Epoch 112/200] [Batch 300/323] [D loss: 0.0004] [G loss: 14.9965]\n",
      "[Epoch 113/200] [Batch 0/323] [D loss: 0.0003] [G loss: 15.0417]\n",
      "[Epoch 113/200] [Batch 100/323] [D loss: 2.0591] [G loss: 1.4659]\n",
      "[Epoch 113/200] [Batch 200/323] [D loss: 0.0030] [G loss: 9.8483]\n",
      "[Epoch 113/200] [Batch 300/323] [D loss: 0.0144] [G loss: 7.9385]\n",
      "[Epoch 114/200] [Batch 0/323] [D loss: 0.0008] [G loss: 8.9635]\n",
      "[Epoch 114/200] [Batch 100/323] [D loss: 0.0003] [G loss: 14.7311]\n",
      "[Epoch 114/200] [Batch 200/323] [D loss: 0.0570] [G loss: 18.4261]\n",
      "[Epoch 114/200] [Batch 300/323] [D loss: 0.0007] [G loss: 13.6600]\n",
      "[Epoch 115/200] [Batch 0/323] [D loss: 0.0057] [G loss: 13.8253]\n",
      "[Epoch 115/200] [Batch 100/323] [D loss: 0.0003] [G loss: 10.7466]\n",
      "[Epoch 115/200] [Batch 200/323] [D loss: 0.0001] [G loss: 12.1736]\n",
      "[Epoch 115/200] [Batch 300/323] [D loss: 0.0002] [G loss: 12.1312]\n",
      "[Epoch 116/200] [Batch 0/323] [D loss: 0.0401] [G loss: 6.1069]\n",
      "[Epoch 116/200] [Batch 100/323] [D loss: 0.0007] [G loss: 12.6179]\n",
      "[Epoch 116/200] [Batch 200/323] [D loss: 0.0088] [G loss: 7.0680]\n",
      "[Epoch 116/200] [Batch 300/323] [D loss: 0.0009] [G loss: 9.8384]\n",
      "[Epoch 117/200] [Batch 0/323] [D loss: 0.0058] [G loss: 6.8918]\n",
      "[Epoch 117/200] [Batch 100/323] [D loss: 0.0000] [G loss: 17.6587]\n",
      "[Epoch 117/200] [Batch 200/323] [D loss: 0.0012] [G loss: 10.8368]\n",
      "[Epoch 117/200] [Batch 300/323] [D loss: 0.0005] [G loss: 9.0021]\n",
      "[Epoch 118/200] [Batch 0/323] [D loss: 0.0001] [G loss: 14.3162]\n",
      "[Epoch 118/200] [Batch 100/323] [D loss: 0.0015] [G loss: 8.4917]\n",
      "[Epoch 118/200] [Batch 200/323] [D loss: 0.0002] [G loss: 18.3788]\n",
      "[Epoch 118/200] [Batch 300/323] [D loss: 0.0000] [G loss: 16.8174]\n",
      "[Epoch 119/200] [Batch 0/323] [D loss: 0.0058] [G loss: 8.0784]\n",
      "[Epoch 119/200] [Batch 100/323] [D loss: 0.0007] [G loss: 9.8357]\n",
      "[Epoch 119/200] [Batch 200/323] [D loss: 0.0058] [G loss: 14.1853]\n",
      "[Epoch 119/200] [Batch 300/323] [D loss: 0.0030] [G loss: 10.8266]\n",
      "[Epoch 120/200] [Batch 0/323] [D loss: 0.0016] [G loss: 11.8873]\n",
      "[Epoch 120/200] [Batch 100/323] [D loss: 0.0001] [G loss: 17.2858]\n",
      "[Epoch 120/200] [Batch 200/323] [D loss: 0.0024] [G loss: 14.4761]\n",
      "[Epoch 120/200] [Batch 300/323] [D loss: 0.0027] [G loss: 11.6350]\n",
      "[Epoch 121/200] [Batch 0/323] [D loss: 0.0003] [G loss: 11.7710]\n",
      "[Epoch 121/200] [Batch 100/323] [D loss: 0.0059] [G loss: 7.3534]\n",
      "[Epoch 121/200] [Batch 200/323] [D loss: 0.0009] [G loss: 13.0373]\n",
      "[Epoch 121/200] [Batch 300/323] [D loss: 0.0000] [G loss: 15.1685]\n",
      "[Epoch 122/200] [Batch 0/323] [D loss: 0.0003] [G loss: 9.8870]\n",
      "[Epoch 122/200] [Batch 100/323] [D loss: 0.0002] [G loss: 14.0940]\n",
      "[Epoch 122/200] [Batch 200/323] [D loss: 0.0002] [G loss: 11.8911]\n",
      "[Epoch 122/200] [Batch 300/323] [D loss: 0.0055] [G loss: 13.0916]\n",
      "[Epoch 123/200] [Batch 0/323] [D loss: 0.0008] [G loss: 9.7851]\n",
      "[Epoch 123/200] [Batch 100/323] [D loss: 0.0011] [G loss: 11.8607]\n",
      "[Epoch 123/200] [Batch 200/323] [D loss: 0.0000] [G loss: 17.8535]\n",
      "[Epoch 123/200] [Batch 300/323] [D loss: 0.0020] [G loss: 7.8372]\n",
      "[Epoch 124/200] [Batch 0/323] [D loss: 0.0001] [G loss: 14.0782]\n",
      "[Epoch 124/200] [Batch 100/323] [D loss: 0.4605] [G loss: 2.0695]\n",
      "[Epoch 124/200] [Batch 200/323] [D loss: 0.0041] [G loss: 17.4537]\n",
      "[Epoch 124/200] [Batch 300/323] [D loss: 0.0002] [G loss: 18.4091]\n",
      "[Epoch 125/200] [Batch 0/323] [D loss: 0.0007] [G loss: 10.1419]\n",
      "[Epoch 125/200] [Batch 100/323] [D loss: 0.0001] [G loss: 14.7137]\n",
      "[Epoch 125/200] [Batch 200/323] [D loss: 0.0029] [G loss: 13.4180]\n",
      "[Epoch 125/200] [Batch 300/323] [D loss: 0.0002] [G loss: 15.6823]\n",
      "[Epoch 126/200] [Batch 0/323] [D loss: 0.0003] [G loss: 11.5054]\n",
      "[Epoch 126/200] [Batch 100/323] [D loss: 0.0002] [G loss: 10.5811]\n",
      "[Epoch 126/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.1798]\n",
      "[Epoch 126/200] [Batch 300/323] [D loss: 0.0013] [G loss: 8.4811]\n",
      "[Epoch 127/200] [Batch 0/323] [D loss: 0.0033] [G loss: 7.6248]\n",
      "[Epoch 127/200] [Batch 100/323] [D loss: 0.0019] [G loss: 9.3097]\n",
      "[Epoch 127/200] [Batch 200/323] [D loss: 0.0001] [G loss: 17.6276]\n",
      "[Epoch 127/200] [Batch 300/323] [D loss: 0.0005] [G loss: 9.8443]\n",
      "[Epoch 128/200] [Batch 0/323] [D loss: 0.0001] [G loss: 13.6258]\n",
      "[Epoch 128/200] [Batch 100/323] [D loss: 0.0033] [G loss: 7.7611]\n",
      "[Epoch 128/200] [Batch 200/323] [D loss: 0.0004] [G loss: 13.9296]\n",
      "[Epoch 128/200] [Batch 300/323] [D loss: 0.0049] [G loss: 7.3214]\n",
      "[Epoch 129/200] [Batch 0/323] [D loss: 0.0002] [G loss: 12.4929]\n",
      "[Epoch 129/200] [Batch 100/323] [D loss: 0.0026] [G loss: 10.2308]\n",
      "[Epoch 129/200] [Batch 200/323] [D loss: 0.0005] [G loss: 10.0359]\n",
      "[Epoch 129/200] [Batch 300/323] [D loss: 0.0069] [G loss: 6.3591]\n",
      "[Epoch 130/200] [Batch 0/323] [D loss: 0.0002] [G loss: 12.8228]\n",
      "[Epoch 130/200] [Batch 100/323] [D loss: 0.0022] [G loss: 17.0346]\n",
      "[Epoch 130/200] [Batch 200/323] [D loss: 0.0190] [G loss: 16.7458]\n",
      "[Epoch 130/200] [Batch 300/323] [D loss: 0.0000] [G loss: 15.1768]\n",
      "[Epoch 131/200] [Batch 0/323] [D loss: 0.0004] [G loss: 12.1581]\n",
      "[Epoch 131/200] [Batch 100/323] [D loss: 0.0002] [G loss: 10.7714]\n",
      "[Epoch 131/200] [Batch 200/323] [D loss: 0.0001] [G loss: 10.8238]\n",
      "[Epoch 131/200] [Batch 300/323] [D loss: 0.0005] [G loss: 10.6401]\n",
      "[Epoch 132/200] [Batch 0/323] [D loss: 0.0183] [G loss: 10.8123]\n",
      "[Epoch 132/200] [Batch 100/323] [D loss: 0.0004] [G loss: 12.2078]\n",
      "[Epoch 132/200] [Batch 200/323] [D loss: 0.0004] [G loss: 32.9245]\n",
      "[Epoch 132/200] [Batch 300/323] [D loss: 0.0590] [G loss: 5.3969]\n",
      "[Epoch 133/200] [Batch 0/323] [D loss: 0.0002] [G loss: 18.1954]\n",
      "[Epoch 133/200] [Batch 100/323] [D loss: 0.0026] [G loss: 12.8528]\n",
      "[Epoch 133/200] [Batch 200/323] [D loss: 0.0004] [G loss: 11.1881]\n",
      "[Epoch 133/200] [Batch 300/323] [D loss: 0.0006] [G loss: 9.2360]\n",
      "[Epoch 134/200] [Batch 0/323] [D loss: 0.0060] [G loss: 7.8318]\n",
      "[Epoch 134/200] [Batch 100/323] [D loss: 0.0004] [G loss: 13.0160]\n",
      "[Epoch 134/200] [Batch 200/323] [D loss: 0.0003] [G loss: 9.7946]\n",
      "[Epoch 134/200] [Batch 300/323] [D loss: 0.0016] [G loss: 9.2923]\n",
      "[Epoch 135/200] [Batch 0/323] [D loss: 0.0001] [G loss: 16.2430]\n",
      "[Epoch 135/200] [Batch 100/323] [D loss: 0.0003] [G loss: 12.6531]\n",
      "[Epoch 135/200] [Batch 200/323] [D loss: 0.0003] [G loss: 11.8763]\n",
      "[Epoch 135/200] [Batch 300/323] [D loss: 0.0004] [G loss: 14.6783]\n",
      "[Epoch 136/200] [Batch 0/323] [D loss: 0.0001] [G loss: 14.4191]\n",
      "[Epoch 136/200] [Batch 100/323] [D loss: 0.0051] [G loss: 16.4821]\n",
      "[Epoch 136/200] [Batch 200/323] [D loss: 0.3884] [G loss: 37.7540]\n",
      "[Epoch 136/200] [Batch 300/323] [D loss: 0.0002] [G loss: 12.6657]\n",
      "[Epoch 137/200] [Batch 0/323] [D loss: 0.0008] [G loss: 9.2741]\n",
      "[Epoch 137/200] [Batch 100/323] [D loss: 0.0026] [G loss: 8.9794]\n",
      "[Epoch 137/200] [Batch 200/323] [D loss: 0.0105] [G loss: 7.5908]\n",
      "[Epoch 137/200] [Batch 300/323] [D loss: 0.0000] [G loss: 12.4107]\n",
      "[Epoch 138/200] [Batch 0/323] [D loss: 0.0030] [G loss: 9.6836]\n",
      "[Epoch 138/200] [Batch 100/323] [D loss: 0.1366] [G loss: 13.6019]\n",
      "[Epoch 138/200] [Batch 200/323] [D loss: 0.0008] [G loss: 12.0334]\n",
      "[Epoch 138/200] [Batch 300/323] [D loss: 0.0008] [G loss: 11.5173]\n",
      "[Epoch 139/200] [Batch 0/323] [D loss: 0.0042] [G loss: 7.4069]\n",
      "[Epoch 139/200] [Batch 100/323] [D loss: 0.0006] [G loss: 17.8053]\n",
      "[Epoch 139/200] [Batch 200/323] [D loss: 0.0002] [G loss: 11.2646]\n",
      "[Epoch 139/200] [Batch 300/323] [D loss: 0.0004] [G loss: 10.0207]\n",
      "[Epoch 140/200] [Batch 0/323] [D loss: 0.0000] [G loss: 15.7487]\n",
      "[Epoch 140/200] [Batch 100/323] [D loss: 0.0000] [G loss: 13.9524]\n",
      "[Epoch 140/200] [Batch 200/323] [D loss: 0.0000] [G loss: 13.2028]\n",
      "[Epoch 140/200] [Batch 300/323] [D loss: 0.0004] [G loss: 11.2127]\n",
      "[Epoch 141/200] [Batch 0/323] [D loss: 0.0127] [G loss: 7.8922]\n",
      "[Epoch 141/200] [Batch 100/323] [D loss: 0.0004] [G loss: 9.7751]\n",
      "[Epoch 141/200] [Batch 200/323] [D loss: 0.0005] [G loss: 11.3982]\n",
      "[Epoch 141/200] [Batch 300/323] [D loss: 0.0048] [G loss: 13.4977]\n",
      "[Epoch 142/200] [Batch 0/323] [D loss: 0.0002] [G loss: 14.7105]\n",
      "[Epoch 142/200] [Batch 100/323] [D loss: 0.0000] [G loss: 12.4389]\n",
      "[Epoch 142/200] [Batch 200/323] [D loss: 0.0002] [G loss: 11.5766]\n",
      "[Epoch 142/200] [Batch 300/323] [D loss: 0.1280] [G loss: 4.5339]\n",
      "[Epoch 143/200] [Batch 0/323] [D loss: 0.1267] [G loss: 12.0890]\n",
      "[Epoch 143/200] [Batch 100/323] [D loss: 0.0180] [G loss: 6.1651]\n",
      "[Epoch 143/200] [Batch 200/323] [D loss: 0.0010] [G loss: 15.9308]\n",
      "[Epoch 143/200] [Batch 300/323] [D loss: 0.0019] [G loss: 10.7287]\n",
      "[Epoch 144/200] [Batch 0/323] [D loss: 0.0002] [G loss: 17.8283]\n",
      "[Epoch 144/200] [Batch 100/323] [D loss: 0.0045] [G loss: 9.9294]\n",
      "[Epoch 144/200] [Batch 200/323] [D loss: 0.0000] [G loss: 13.3649]\n",
      "[Epoch 144/200] [Batch 300/323] [D loss: 0.0000] [G loss: 12.3932]\n",
      "[Epoch 145/200] [Batch 0/323] [D loss: 0.0083] [G loss: 6.6532]\n",
      "[Epoch 145/200] [Batch 100/323] [D loss: 0.0012] [G loss: 8.4549]\n",
      "[Epoch 145/200] [Batch 200/323] [D loss: 0.0025] [G loss: 10.9696]\n",
      "[Epoch 145/200] [Batch 300/323] [D loss: 0.0004] [G loss: 11.0012]\n",
      "[Epoch 146/200] [Batch 0/323] [D loss: 0.0031] [G loss: 15.5878]\n",
      "[Epoch 146/200] [Batch 100/323] [D loss: 0.0005] [G loss: 11.9802]\n",
      "[Epoch 146/200] [Batch 200/323] [D loss: 0.0125] [G loss: 7.1387]\n",
      "[Epoch 146/200] [Batch 300/323] [D loss: 0.0031] [G loss: 19.5716]\n",
      "[Epoch 147/200] [Batch 0/323] [D loss: 0.0017] [G loss: 8.7164]\n",
      "[Epoch 147/200] [Batch 100/323] [D loss: 0.0000] [G loss: 18.4487]\n",
      "[Epoch 147/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.9491]\n",
      "[Epoch 147/200] [Batch 300/323] [D loss: 0.0005] [G loss: 16.2287]\n",
      "[Epoch 148/200] [Batch 0/323] [D loss: 0.0011] [G loss: 19.5304]\n",
      "[Epoch 148/200] [Batch 100/323] [D loss: 0.0034] [G loss: 10.0523]\n",
      "[Epoch 148/200] [Batch 200/323] [D loss: 0.0001] [G loss: 11.2032]\n",
      "[Epoch 148/200] [Batch 300/323] [D loss: 0.0002] [G loss: 15.3604]\n",
      "[Epoch 149/200] [Batch 0/323] [D loss: 0.0002] [G loss: 16.6136]\n",
      "[Epoch 149/200] [Batch 100/323] [D loss: 0.0010] [G loss: 10.8201]\n",
      "[Epoch 149/200] [Batch 200/323] [D loss: 0.0006] [G loss: 16.7467]\n",
      "[Epoch 149/200] [Batch 300/323] [D loss: 0.0001] [G loss: 15.7519]\n",
      "[Epoch 150/200] [Batch 0/323] [D loss: 0.0000] [G loss: 16.3019]\n",
      "[Epoch 150/200] [Batch 100/323] [D loss: 0.0607] [G loss: 22.2069]\n",
      "[Epoch 150/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.6046]\n",
      "[Epoch 150/200] [Batch 300/323] [D loss: 0.0123] [G loss: 25.9478]\n",
      "[Epoch 151/200] [Batch 0/323] [D loss: 0.1564] [G loss: 15.9904]\n",
      "[Epoch 151/200] [Batch 100/323] [D loss: 0.0051] [G loss: 8.6059]\n",
      "[Epoch 151/200] [Batch 200/323] [D loss: 0.0052] [G loss: 15.6111]\n",
      "[Epoch 151/200] [Batch 300/323] [D loss: 0.0001] [G loss: 13.4077]\n",
      "[Epoch 152/200] [Batch 0/323] [D loss: 0.0002] [G loss: 14.9628]\n",
      "[Epoch 152/200] [Batch 100/323] [D loss: 0.0008] [G loss: 13.4945]\n",
      "[Epoch 152/200] [Batch 200/323] [D loss: 0.0000] [G loss: 13.0117]\n",
      "[Epoch 152/200] [Batch 300/323] [D loss: 0.0003] [G loss: 15.3148]\n",
      "[Epoch 153/200] [Batch 0/323] [D loss: 0.0001] [G loss: 14.4532]\n",
      "[Epoch 153/200] [Batch 100/323] [D loss: 0.0014] [G loss: 21.7644]\n",
      "[Epoch 153/200] [Batch 200/323] [D loss: 0.0001] [G loss: 15.4772]\n",
      "[Epoch 153/200] [Batch 300/323] [D loss: 0.0004] [G loss: 10.4700]\n",
      "[Epoch 154/200] [Batch 0/323] [D loss: 0.0000] [G loss: 16.8536]\n",
      "[Epoch 154/200] [Batch 100/323] [D loss: 0.0000] [G loss: 14.6596]\n",
      "[Epoch 154/200] [Batch 200/323] [D loss: 0.0001] [G loss: 12.9722]\n",
      "[Epoch 154/200] [Batch 300/323] [D loss: 0.0000] [G loss: 13.4514]\n",
      "[Epoch 155/200] [Batch 0/323] [D loss: 0.0000] [G loss: 16.9221]\n",
      "[Epoch 155/200] [Batch 100/323] [D loss: 0.0002] [G loss: 19.1377]\n",
      "[Epoch 155/200] [Batch 200/323] [D loss: 0.0015] [G loss: 9.2909]\n",
      "[Epoch 155/200] [Batch 300/323] [D loss: 0.0001] [G loss: 12.5447]\n",
      "[Epoch 156/200] [Batch 0/323] [D loss: 0.0011] [G loss: 12.7278]\n",
      "[Epoch 156/200] [Batch 100/323] [D loss: 0.0002] [G loss: 16.0261]\n",
      "[Epoch 156/200] [Batch 200/323] [D loss: 0.0042] [G loss: 8.2906]\n",
      "[Epoch 156/200] [Batch 300/323] [D loss: 0.0013] [G loss: 11.2335]\n",
      "[Epoch 157/200] [Batch 0/323] [D loss: 0.2986] [G loss: 23.6271]\n",
      "[Epoch 157/200] [Batch 100/323] [D loss: 0.0041] [G loss: 9.0016]\n",
      "[Epoch 157/200] [Batch 200/323] [D loss: 0.0009] [G loss: 10.2446]\n",
      "[Epoch 157/200] [Batch 300/323] [D loss: 0.0086] [G loss: 15.7600]\n",
      "[Epoch 158/200] [Batch 0/323] [D loss: 0.0001] [G loss: 16.9225]\n",
      "[Epoch 158/200] [Batch 100/323] [D loss: 0.0029] [G loss: 14.5451]\n",
      "[Epoch 158/200] [Batch 200/323] [D loss: 0.0000] [G loss: 15.4109]\n",
      "[Epoch 158/200] [Batch 300/323] [D loss: 0.0003] [G loss: 15.2990]\n",
      "[Epoch 159/200] [Batch 0/323] [D loss: 0.0005] [G loss: 9.5006]\n",
      "[Epoch 159/200] [Batch 100/323] [D loss: 0.0006] [G loss: 14.6419]\n",
      "[Epoch 159/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.1810]\n",
      "[Epoch 159/200] [Batch 300/323] [D loss: 0.0000] [G loss: 15.4022]\n",
      "[Epoch 160/200] [Batch 0/323] [D loss: 0.0003] [G loss: 10.6392]\n",
      "[Epoch 160/200] [Batch 100/323] [D loss: 0.0009] [G loss: 11.8880]\n",
      "[Epoch 160/200] [Batch 200/323] [D loss: 0.0005] [G loss: 17.6698]\n",
      "[Epoch 160/200] [Batch 300/323] [D loss: 0.0204] [G loss: 11.3121]\n",
      "[Epoch 161/200] [Batch 0/323] [D loss: 0.0120] [G loss: 13.4066]\n",
      "[Epoch 161/200] [Batch 100/323] [D loss: 0.0001] [G loss: 13.6792]\n",
      "[Epoch 161/200] [Batch 200/323] [D loss: 0.0060] [G loss: 9.7948]\n",
      "[Epoch 161/200] [Batch 300/323] [D loss: 0.0002] [G loss: 11.6400]\n",
      "[Epoch 162/200] [Batch 0/323] [D loss: 0.0009] [G loss: 8.9956]\n",
      "[Epoch 162/200] [Batch 100/323] [D loss: 0.0001] [G loss: 11.1370]\n",
      "[Epoch 162/200] [Batch 200/323] [D loss: 0.0001] [G loss: 11.2339]\n",
      "[Epoch 162/200] [Batch 300/323] [D loss: 0.0015] [G loss: 8.9206]\n",
      "[Epoch 163/200] [Batch 0/323] [D loss: 0.0068] [G loss: 11.4067]\n",
      "[Epoch 163/200] [Batch 100/323] [D loss: 0.0001] [G loss: 12.9816]\n",
      "[Epoch 163/200] [Batch 200/323] [D loss: 0.0006] [G loss: 12.1505]\n",
      "[Epoch 163/200] [Batch 300/323] [D loss: 0.1429] [G loss: 14.7042]\n",
      "[Epoch 164/200] [Batch 0/323] [D loss: 0.0569] [G loss: 13.1119]\n",
      "[Epoch 164/200] [Batch 100/323] [D loss: 0.0024] [G loss: 11.3463]\n",
      "[Epoch 164/200] [Batch 200/323] [D loss: 0.0002] [G loss: 10.5177]\n",
      "[Epoch 164/200] [Batch 300/323] [D loss: 0.0003] [G loss: 14.4104]\n",
      "[Epoch 165/200] [Batch 0/323] [D loss: 0.0000] [G loss: 13.5704]\n",
      "[Epoch 165/200] [Batch 100/323] [D loss: 0.0055] [G loss: 8.5829]\n",
      "[Epoch 165/200] [Batch 200/323] [D loss: 0.0013] [G loss: 9.1877]\n",
      "[Epoch 165/200] [Batch 300/323] [D loss: 0.0002] [G loss: 11.8567]\n",
      "[Epoch 166/200] [Batch 0/323] [D loss: 0.0002] [G loss: 15.6252]\n",
      "[Epoch 166/200] [Batch 100/323] [D loss: 0.0003] [G loss: 12.6628]\n",
      "[Epoch 166/200] [Batch 200/323] [D loss: 0.0009] [G loss: 9.4758]\n",
      "[Epoch 166/200] [Batch 300/323] [D loss: 0.0001] [G loss: 12.3155]\n",
      "[Epoch 167/200] [Batch 0/323] [D loss: 0.0002] [G loss: 11.7854]\n",
      "[Epoch 167/200] [Batch 100/323] [D loss: 0.0000] [G loss: 14.8404]\n",
      "[Epoch 167/200] [Batch 200/323] [D loss: 0.0080] [G loss: 11.8148]\n",
      "[Epoch 167/200] [Batch 300/323] [D loss: 0.0124] [G loss: 7.2990]\n",
      "[Epoch 168/200] [Batch 0/323] [D loss: 0.0022] [G loss: 8.0459]\n",
      "[Epoch 168/200] [Batch 100/323] [D loss: 0.0084] [G loss: 17.9794]\n",
      "[Epoch 168/200] [Batch 200/323] [D loss: 0.0000] [G loss: 13.3609]\n",
      "[Epoch 168/200] [Batch 300/323] [D loss: 0.0020] [G loss: 8.4743]\n",
      "[Epoch 169/200] [Batch 0/323] [D loss: 0.0008] [G loss: 10.5370]\n",
      "[Epoch 169/200] [Batch 100/323] [D loss: 0.0027] [G loss: 16.7169]\n",
      "[Epoch 169/200] [Batch 200/323] [D loss: 0.0018] [G loss: 14.2564]\n",
      "[Epoch 169/200] [Batch 300/323] [D loss: 0.0000] [G loss: 15.6059]\n",
      "[Epoch 170/200] [Batch 0/323] [D loss: 0.0001] [G loss: 11.7475]\n",
      "[Epoch 170/200] [Batch 100/323] [D loss: 0.0425] [G loss: 7.5019]\n",
      "[Epoch 170/200] [Batch 200/323] [D loss: 0.0006] [G loss: 14.4315]\n",
      "[Epoch 170/200] [Batch 300/323] [D loss: 0.0100] [G loss: 13.9317]\n",
      "[Epoch 171/200] [Batch 0/323] [D loss: 0.0008] [G loss: 10.4130]\n",
      "[Epoch 171/200] [Batch 100/323] [D loss: 0.0008] [G loss: 12.4451]\n",
      "[Epoch 171/200] [Batch 200/323] [D loss: 0.0041] [G loss: 7.6921]\n",
      "[Epoch 171/200] [Batch 300/323] [D loss: 0.0041] [G loss: 14.6470]\n",
      "[Epoch 172/200] [Batch 0/323] [D loss: 0.0017] [G loss: 17.6614]\n",
      "[Epoch 172/200] [Batch 100/323] [D loss: 0.0077] [G loss: 7.1354]\n",
      "[Epoch 172/200] [Batch 200/323] [D loss: 0.0044] [G loss: 11.6738]\n",
      "[Epoch 172/200] [Batch 300/323] [D loss: 0.0018] [G loss: 11.0344]\n",
      "[Epoch 173/200] [Batch 0/323] [D loss: 0.0016] [G loss: 14.2327]\n",
      "[Epoch 173/200] [Batch 100/323] [D loss: 0.0000] [G loss: 13.8561]\n",
      "[Epoch 173/200] [Batch 200/323] [D loss: 0.0013] [G loss: 12.3683]\n",
      "[Epoch 173/200] [Batch 300/323] [D loss: 0.0023] [G loss: 9.0747]\n",
      "[Epoch 174/200] [Batch 0/323] [D loss: 0.0006] [G loss: 10.2465]\n",
      "[Epoch 174/200] [Batch 100/323] [D loss: 0.0002] [G loss: 11.4152]\n",
      "[Epoch 174/200] [Batch 200/323] [D loss: 0.0000] [G loss: 13.0735]\n",
      "[Epoch 174/200] [Batch 300/323] [D loss: 0.0001] [G loss: 12.1262]\n",
      "[Epoch 175/200] [Batch 0/323] [D loss: 0.0003] [G loss: 11.3579]\n",
      "[Epoch 175/200] [Batch 100/323] [D loss: 0.2977] [G loss: 3.3145]\n",
      "[Epoch 175/200] [Batch 200/323] [D loss: 0.0007] [G loss: 9.5838]\n",
      "[Epoch 175/200] [Batch 300/323] [D loss: 0.0000] [G loss: 15.1583]\n",
      "[Epoch 176/200] [Batch 0/323] [D loss: 0.0000] [G loss: 18.6552]\n",
      "[Epoch 176/200] [Batch 100/323] [D loss: 0.0005] [G loss: 10.1915]\n",
      "[Epoch 176/200] [Batch 200/323] [D loss: 0.0000] [G loss: 16.8735]\n",
      "[Epoch 176/200] [Batch 300/323] [D loss: 0.0000] [G loss: 19.1230]\n",
      "[Epoch 177/200] [Batch 0/323] [D loss: 0.0001] [G loss: 19.5556]\n",
      "[Epoch 177/200] [Batch 100/323] [D loss: 0.0234] [G loss: 8.4153]\n",
      "[Epoch 177/200] [Batch 200/323] [D loss: 0.0109] [G loss: 7.5244]\n",
      "[Epoch 177/200] [Batch 300/323] [D loss: 0.0006] [G loss: 10.1118]\n",
      "[Epoch 178/200] [Batch 0/323] [D loss: 0.0027] [G loss: 9.8907]\n",
      "[Epoch 178/200] [Batch 100/323] [D loss: 0.0001] [G loss: 13.9703]\n",
      "[Epoch 178/200] [Batch 200/323] [D loss: 0.0011] [G loss: 11.9626]\n",
      "[Epoch 178/200] [Batch 300/323] [D loss: 0.0000] [G loss: 14.7914]\n",
      "[Epoch 179/200] [Batch 0/323] [D loss: 0.0013] [G loss: 12.4118]\n",
      "[Epoch 179/200] [Batch 100/323] [D loss: 0.0012] [G loss: 9.6665]\n",
      "[Epoch 179/200] [Batch 200/323] [D loss: 0.0006] [G loss: 9.2684]\n",
      "[Epoch 179/200] [Batch 300/323] [D loss: 0.0001] [G loss: 13.9642]\n",
      "[Epoch 180/200] [Batch 0/323] [D loss: 0.0000] [G loss: 18.0457]\n",
      "[Epoch 180/200] [Batch 100/323] [D loss: 0.0472] [G loss: 16.5124]\n",
      "[Epoch 180/200] [Batch 200/323] [D loss: 0.0016] [G loss: 10.4196]\n",
      "[Epoch 180/200] [Batch 300/323] [D loss: 0.0000] [G loss: 14.4280]\n",
      "[Epoch 181/200] [Batch 0/323] [D loss: 0.0011] [G loss: 11.6180]\n",
      "[Epoch 181/200] [Batch 100/323] [D loss: 0.0716] [G loss: 6.2136]\n",
      "[Epoch 181/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.5773]\n",
      "[Epoch 181/200] [Batch 300/323] [D loss: 0.0001] [G loss: 16.0195]\n",
      "[Epoch 182/200] [Batch 0/323] [D loss: 0.0021] [G loss: 8.2516]\n",
      "[Epoch 182/200] [Batch 100/323] [D loss: 0.0000] [G loss: 15.5185]\n",
      "[Epoch 182/200] [Batch 200/323] [D loss: 0.0000] [G loss: 12.0806]\n",
      "[Epoch 182/200] [Batch 300/323] [D loss: 0.0002] [G loss: 12.7402]\n",
      "[Epoch 183/200] [Batch 0/323] [D loss: 0.0001] [G loss: 13.0841]\n",
      "[Epoch 183/200] [Batch 100/323] [D loss: 0.0006] [G loss: 20.4321]\n",
      "[Epoch 183/200] [Batch 200/323] [D loss: 0.0001] [G loss: 13.1005]\n",
      "[Epoch 183/200] [Batch 300/323] [D loss: 0.0018] [G loss: 19.3779]\n",
      "[Epoch 184/200] [Batch 0/323] [D loss: 0.0000] [G loss: 17.3714]\n",
      "[Epoch 184/200] [Batch 100/323] [D loss: 0.0008] [G loss: 15.4727]\n",
      "[Epoch 184/200] [Batch 200/323] [D loss: 0.0000] [G loss: 21.1018]\n",
      "[Epoch 184/200] [Batch 300/323] [D loss: 0.0001] [G loss: 11.7869]\n",
      "[Epoch 185/200] [Batch 0/323] [D loss: 0.0000] [G loss: 13.5233]\n",
      "[Epoch 185/200] [Batch 100/323] [D loss: 0.0000] [G loss: 18.1346]\n",
      "[Epoch 185/200] [Batch 200/323] [D loss: 0.0026] [G loss: 17.9200]\n",
      "[Epoch 185/200] [Batch 300/323] [D loss: 0.0386] [G loss: 6.1012]\n",
      "[Epoch 186/200] [Batch 0/323] [D loss: 0.0006] [G loss: 14.4513]\n",
      "[Epoch 186/200] [Batch 100/323] [D loss: 0.0000] [G loss: 16.9640]\n",
      "[Epoch 186/200] [Batch 200/323] [D loss: 0.3408] [G loss: 4.5387]\n",
      "[Epoch 186/200] [Batch 300/323] [D loss: 0.0002] [G loss: 10.9821]\n",
      "[Epoch 187/200] [Batch 0/323] [D loss: 0.0000] [G loss: 16.4291]\n",
      "[Epoch 187/200] [Batch 100/323] [D loss: 0.0058] [G loss: 14.6533]\n",
      "[Epoch 187/200] [Batch 200/323] [D loss: 0.0100] [G loss: 12.9192]\n",
      "[Epoch 187/200] [Batch 300/323] [D loss: 0.0006] [G loss: 12.2652]\n",
      "[Epoch 188/200] [Batch 0/323] [D loss: 0.0013] [G loss: 12.1528]\n",
      "[Epoch 188/200] [Batch 100/323] [D loss: 0.0001] [G loss: 12.2538]\n",
      "[Epoch 188/200] [Batch 200/323] [D loss: 0.0024] [G loss: 13.4530]\n",
      "[Epoch 188/200] [Batch 300/323] [D loss: 0.0010] [G loss: 15.4417]\n",
      "[Epoch 189/200] [Batch 0/323] [D loss: 0.0001] [G loss: 15.9523]\n",
      "[Epoch 189/200] [Batch 100/323] [D loss: 0.0057] [G loss: 9.8899]\n",
      "[Epoch 189/200] [Batch 200/323] [D loss: 0.0007] [G loss: 10.2372]\n",
      "[Epoch 189/200] [Batch 300/323] [D loss: 0.0000] [G loss: 16.9314]\n",
      "[Epoch 190/200] [Batch 0/323] [D loss: 0.0001] [G loss: 14.8951]\n",
      "[Epoch 190/200] [Batch 100/323] [D loss: 0.0003] [G loss: 16.4951]\n",
      "[Epoch 190/200] [Batch 200/323] [D loss: 0.0007] [G loss: 11.6088]\n",
      "[Epoch 190/200] [Batch 300/323] [D loss: 0.0001] [G loss: 12.4748]\n",
      "[Epoch 191/200] [Batch 0/323] [D loss: 0.0004] [G loss: 10.7876]\n",
      "[Epoch 191/200] [Batch 100/323] [D loss: 0.0000] [G loss: 17.2412]\n",
      "[Epoch 191/200] [Batch 200/323] [D loss: 0.0029] [G loss: 8.9223]\n",
      "[Epoch 191/200] [Batch 300/323] [D loss: 0.0004] [G loss: 11.8800]\n",
      "[Epoch 192/200] [Batch 0/323] [D loss: 0.0088] [G loss: 14.1657]\n",
      "[Epoch 192/200] [Batch 100/323] [D loss: 0.0086] [G loss: 7.0712]\n",
      "[Epoch 192/200] [Batch 200/323] [D loss: 0.0020] [G loss: 8.9814]\n",
      "[Epoch 192/200] [Batch 300/323] [D loss: 0.0002] [G loss: 13.0783]\n",
      "[Epoch 193/200] [Batch 0/323] [D loss: 0.0006] [G loss: 14.4077]\n",
      "[Epoch 193/200] [Batch 100/323] [D loss: 0.0001] [G loss: 13.2411]\n",
      "[Epoch 193/200] [Batch 200/323] [D loss: 0.0001] [G loss: 11.7432]\n",
      "[Epoch 193/200] [Batch 300/323] [D loss: 0.0000] [G loss: 16.7596]\n",
      "[Epoch 194/200] [Batch 0/323] [D loss: 0.0135] [G loss: 14.8558]\n",
      "[Epoch 194/200] [Batch 100/323] [D loss: 0.0000] [G loss: 15.1118]\n",
      "[Epoch 194/200] [Batch 200/323] [D loss: 0.0020] [G loss: 9.9605]\n",
      "[Epoch 194/200] [Batch 300/323] [D loss: 0.0000] [G loss: 14.5105]\n",
      "[Epoch 195/200] [Batch 0/323] [D loss: 0.0029] [G loss: 15.9407]\n",
      "[Epoch 195/200] [Batch 100/323] [D loss: 0.0001] [G loss: 12.3582]\n",
      "[Epoch 195/200] [Batch 200/323] [D loss: 0.0025] [G loss: 12.3840]\n",
      "[Epoch 195/200] [Batch 300/323] [D loss: 0.0016] [G loss: 15.6552]\n",
      "[Epoch 196/200] [Batch 0/323] [D loss: 0.0004] [G loss: 10.9201]\n",
      "[Epoch 196/200] [Batch 100/323] [D loss: 0.0001] [G loss: 13.7853]\n",
      "[Epoch 196/200] [Batch 200/323] [D loss: 0.0000] [G loss: 16.7526]\n",
      "[Epoch 196/200] [Batch 300/323] [D loss: 0.0002] [G loss: 11.4209]\n",
      "[Epoch 197/200] [Batch 0/323] [D loss: 0.0000] [G loss: 17.1618]\n",
      "[Epoch 197/200] [Batch 100/323] [D loss: 0.0005] [G loss: 12.1385]\n",
      "[Epoch 197/200] [Batch 200/323] [D loss: 0.0001] [G loss: 15.1071]\n",
      "[Epoch 197/200] [Batch 300/323] [D loss: 0.0001] [G loss: 15.4403]\n",
      "[Epoch 198/200] [Batch 0/323] [D loss: 0.0003] [G loss: 11.4515]\n",
      "[Epoch 198/200] [Batch 100/323] [D loss: 0.0000] [G loss: 12.9729]\n",
      "[Epoch 198/200] [Batch 200/323] [D loss: 0.0047] [G loss: 8.1065]\n",
      "[Epoch 198/200] [Batch 300/323] [D loss: 0.0517] [G loss: 4.7989]\n",
      "[Epoch 199/200] [Batch 0/323] [D loss: 0.0177] [G loss: 12.4162]\n",
      "[Epoch 199/200] [Batch 100/323] [D loss: 0.0297] [G loss: 12.1305]\n",
      "[Epoch 199/200] [Batch 200/323] [D loss: 0.0001] [G loss: 16.7482]\n",
      "[Epoch 199/200] [Batch 300/323] [D loss: 0.0000] [G loss: 15.8540]\n"
     ]
    }
   ],
   "source": [
    "# For training\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0d15eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1149373/1281071737.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3519, -0.4555, -0.7304,  ..., -0.9049, -0.7889, -0.6978],\n",
       "          [-0.3437, -0.4842, -0.4858,  ..., -0.8872, -0.6428, -0.7665],\n",
       "          [-0.4431, -0.2874, -0.2692,  ..., -0.9008, -0.8853, -0.7058],\n",
       "          ...,\n",
       "          [ 0.8666,  0.8406,  0.8020,  ...,  0.8288,  0.7549,  0.7914],\n",
       "          [ 0.9003,  0.8660,  0.8699,  ...,  0.8830,  0.8250,  0.8737],\n",
       "          [ 0.8118,  0.7935,  0.8548,  ...,  0.9584,  0.8868,  0.8340]],\n",
       "\n",
       "         [[-0.1771, -0.1356, -0.4497,  ..., -0.5185, -0.5195, -0.2646],\n",
       "          [-0.1665, -0.4211, -0.6521,  ..., -0.6834, -0.5058, -0.6442],\n",
       "          [-0.2465, -0.1737, -0.5108,  ..., -0.7988, -0.2872, -0.4867],\n",
       "          ...,\n",
       "          [ 0.8510,  0.7742,  0.8802,  ...,  0.6724,  0.6564,  0.4917],\n",
       "          [ 0.8790,  0.8711,  0.8904,  ...,  0.8169,  0.7204,  0.7035],\n",
       "          [ 0.7299,  0.8462,  0.8252,  ...,  0.9093,  0.8975,  0.7782]],\n",
       "\n",
       "         [[-0.1894, -0.2082, -0.7538,  ..., -0.5733, -0.5413, -0.1853],\n",
       "          [-0.0874, -0.3652, -0.5072,  ..., -0.7263, -0.5825, -0.4918],\n",
       "          [-0.1594,  0.0878, -0.4485,  ..., -0.8610, -0.6591, -0.6564],\n",
       "          ...,\n",
       "          [ 0.7460,  0.7866,  0.8291,  ...,  0.7640,  0.6828,  0.7767],\n",
       "          [ 0.8748,  0.9154,  0.8687,  ...,  0.7622,  0.6688,  0.6126],\n",
       "          [ 0.5860,  0.8535,  0.8858,  ...,  0.9280,  0.9143,  0.7883]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For inference\n",
    "model_path = \"checkpoints/checkpoint_epoch_200.pth\"\n",
    "input_image = \"archive/train/photos/0.jpg\"\n",
    "output_path = \"sketch.jpg\"\n",
    "generate_sketch(model_path, input_image, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee367f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
