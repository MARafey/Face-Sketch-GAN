{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils"
   ],
   "id": "62b6ce7fba232677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=1):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.init_size = 64 // 4\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 128 * self.init_size ** 2)\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, condition):\n",
    "        gen_input = torch.cat((noise, condition), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ],
   "id": "5a8008906d6d632b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, condition_dim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                     nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(3 + condition_dim, 64, bn=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "        )\n",
    "\n",
    "        ds_size = 64 // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(512 * ds_size ** 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, condition):\n",
    "        condition = condition.view(condition.size(0), 1, 64, 64)\n",
    "        d_in = torch.cat((img, condition), 1)\n",
    "        out = self.conv_blocks(d_in)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity"
   ],
   "id": "4c74b8bdb4705b2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FaceSketchDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.photo_dir = os.path.join(root_dir, split, 'photos')\n",
    "        self.sketch_dir = os.path.join(root_dir, split, 'sketches')\n",
    "\n",
    "        self.photos = sorted([f for f in os.listdir(self.photo_dir) if f.endswith(('.jpg', '.png'))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.photos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        photo_name = self.photos[idx]\n",
    "        photo_path = os.path.join(self.photo_dir, photo_name)\n",
    "        sketch_path = os.path.join(self.sketch_dir, photo_name)\n",
    "\n",
    "        photo = Image.open(photo_path).convert('RGB')\n",
    "        sketch = Image.open(sketch_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            photo = self.transform(photo)\n",
    "            sketch = self.transform(sketch)\n",
    "\n",
    "        return sketch, photo"
   ],
   "id": "8215c8ee6c674933"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_model(generator, discriminator, epoch, optimizer_G, optimizer_D, path='checkpoints'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "    }, os.path.join(path, f'checkpoint_epoch_{epoch}.pth'))\n",
    "\n",
    "def load_model(generator, discriminator, optimizer_G, optimizer_D, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    return checkpoint['epoch']"
   ],
   "id": "5a77c292985270e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_cgan(generator, discriminator, dataloader, num_epochs, device, save_interval=10):\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    # Move models to device\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    adversarial_loss = adversarial_loss.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sketches, real_imgs) in enumerate(dataloader):\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # Configure input\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            sketches = sketches.to(device)\n",
    "\n",
    "            # Valid and fake labels\n",
    "            valid = Variable(torch.ones(batch_size, 1), requires_grad=False).to(device)\n",
    "            fake = Variable(torch.zeros(batch_size, 1), requires_grad=False).to(device)\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise and generate images\n",
    "            z = Variable(torch.randn(batch_size, generator.latent_dim)).to(device)\n",
    "            gen_imgs = generator(z, sketches)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs, sketches), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Calculate loss for real images\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs, sketches), valid)\n",
    "\n",
    "            # Calculate loss for fake images\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), sketches), fake)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "                # Save sample images\n",
    "                if i % 500 == 0:\n",
    "                    vutils.save_image(gen_imgs.data[:25],\n",
    "                                      f'images/epoch_{epoch}_batch_{i}.png',\n",
    "                                      normalize=True,\n",
    "                                      nrow=5)\n",
    "\n",
    "        # Save model checkpoints\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(generator, discriminator, epoch + 1, optimizer_G, optimizer_D)"
   ],
   "id": "3b9dfd6e89319ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_sketch(model_path, image_path, output_path, device='cuda'):\n",
    "    # Initialize model\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    # Load trained model\n",
    "    load_model(generator, discriminator, optimizer_G, optimizer_D, model_path)\n",
    "    generator.to(device)\n",
    "    generator.eval()\n",
    "\n",
    "    # Prepare image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate sketch\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1, generator.latent_dim).to(device)\n",
    "        generated = generator(z, image)\n",
    "\n",
    "    # Save generated image\n",
    "    vutils.save_image(generated, output_path, normalize=True)\n",
    "    return generated"
   ],
   "id": "d2016690d55686c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    latent_dim = 100\n",
    "    batch_size = 64\n",
    "    num_epochs = 200\n",
    "    image_size = 64\n",
    "    root_dir = \"archive\"  # Update with your dataset path\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create directories for saving results\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = FaceSketchDataset(root_dir, split='train', transform=transform)\n",
    "    val_dataset = FaceSketchDataset(root_dir, split='val', transform=transform)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize models\n",
    "    generator = Generator(latent_dim)\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    # Train the model\n",
    "    train_cgan(generator, discriminator, train_loader, num_epochs, device)"
   ],
   "id": "9eb4c1a4c2a7e3d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For training\n",
    "main()\n",
    "\n",
    "# For inference\n",
    "# model_path = \"checkpoints/checkpoint_epoch_100.pth\"\n",
    "# input_image = \"path/to/input/image.jpg\"\n",
    "# output_path = \"path/to/output/sketch.jpg\"\n",
    "# generate_sketch(model_path, input_image, output_path)"
   ],
   "id": "d6fd9f8e766a87cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
